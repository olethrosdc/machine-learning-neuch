#+TITLE: Linear Regression
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {B}
#+LaTeX_HEADER: \newcommand \param {\beta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\beta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{B}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \by {\vectorsym{y}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
#+OPTIONS:   H:3
* The Linear Model
** Simple linear regression
*** Simple linear regression
**** Input and output
- Data pairs $(x_t, y_t)$, $t = 1, \ldots, T$.
- Input $x_t \in \Reals$
- Output $y_t \in \Reals$.
**** Modelling the conditional expectation $\E[y_t | x_t]$
- Parameters $\param_0, \param_1 \in \Reals$
- Function $\pi_\param : \Reals \to \Reals$, defined as
\[
\pi_\param(x_t) = \param_0 + \param_1 x_{t}
\]
**** Probabilistic predictions: Modelling the conditional probability $\Pr[y_t | x_t]$
- $y_t = \E[y_t | x_t] + \epsilon_t$, with $\epsilon_t \in \Reals$ being zero-mean *noise*.
- Simplest model: variance $\sigma = \Var(\epsilon) =  \E[\epsilon_t^2]$
  
*** Linear models
\begin{tikzpicture}[domain=-1:3]
   \draw[dotted, color=gray] (-1.1,-1.1) grid (5.1,4.1);
   \draw[->] (0,0) -- (4,0) node[right] {$x$};
   \draw[->] (0,0) -- (0,4) node[above] {$y$};
   \draw[thick, color=blue]   plot (\x, {0 + \x * 1})  node[right] {$\beta = (0, 1)$};
   \draw[thick, color=magenta]   plot (\x, {1 - \x * 1/2})  node[right] {$\beta = (1, - 1/2)$};
   \draw[thick, color=red]   plot (\x, {1 - \x * 0})  node[right] {$\beta = (1,  0)$};
\end{tikzpicture}
\[
\pi_\param(x) = \param_0 + \param_1 x = [\param_0, \param_1] \begin{bmatrix}1\\x\end{bmatrix}
\]

*** Two views of the problem
**** Learning as optimisation
#+ATTR_BEAMER: :overlay <+->
- Each value $\pi_\param(x)$ is a *prediction* about the value of $y$
- We suffer a *loss* $\ell(y, \pi_\param(x))$ for every example $(x,y)$ that we see.
- We *can* minimise the average loss over the data 
- Ideally, we *want* to minimise the expected loss, with respect to the unknown distribution $P$.
**** Learning as inference
#+ATTR_BEAMER: :overlay <+->
- The parameters $\param$ define a *probabilistic model* $P_\param(y | x)$ for every value of $y$.
- We want to find the parameters giving the highest *probability* on the observed data.
- Ideally, we want to find the *true conditional distribution $P(y | x)$*.

*** Learning as Optimisation 
Find the parameters $\param$ minimising squared error
\[
\min_\param \frac{1}{T} \sum_{t=1}^T \big[\underbrace{y_t - \pi_\param(x_t)}_{\textrm{residual}}\big]^2
\]
[[../fig/linear_regression_fit.pdf]]

** History
*** Origins
**** Gauss Leg                                                        :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
     #+caption: Gauss: originator
#+attr_latex: :width 100pt
[[../fig/gauss.jpg]]
**** First application                                                :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :END:
#+caption: Legendre: first publication
#+attr_latex: :width 100pt
[[../fig/Legendre.jpg]]
*** The orbit of Ceres
[[../fig/ceres.jpg]]



*** Maximum likelihood inference
**** Gaussian noise model:
\[
y_t = f(x_t) + \epsilon_t,  \qquad \epsilon_t \sim \Normal(0, \sigma)
\]
With conditional density
\[
p_\param(y_t| x_t) 
\propto
\exp(-[y_t - \pi_\param(x_t)]^2 / 2\sigma^2)
\]
#+BEAMER: \pause
**** Maximum likelihood inference
*Idea: For data $D$, find parameters maximising $P_\beta(D)$*
#+BEAMER: \pause
\begin{align*}
\argmax_\param P_\beta(D) 
&= 
\argmax_\param p_\param(y_1, \ldots, y_t| x_1, \ldots, x_T) 
=
\argmax_\param \ln \prod_t p_\param(y_t| x_t) \\
&=
\argmax_\param \sum \ln p_\param(y_t| x_t) \\
&= \argmax_\param \sum_t \ln\left\{\exp\left(-[y_t - \pi_\param(x_t)]^2 / 2\sigma^2\right)\right\}\\
&= \argmax_\param \sum_t -[y_t - \pi_\param(x_t)]^2 / 2\sigma^2
= \argmin_\param \sum_t |y_t - \pi_\param(x_t)|^2
\end{align*}
*** Coding break
src/Regression/RegressionDemonstration.ipynb
- Show implementation
- Fit and residuals
- Multiple draws from the distribution
- Fit on non-linear data?

** Multiple linear regression
*** Multiple linear regression
**** Input and output
- Data pairs $(x_t, y_t)$, $t = 1, \ldots, T$.
- Input $x_t \in \Reals^n$
- Output $y_t \in \Reals^m$.
#+BEAMER: \pause
**** Point predictions: Modelling the conditional expectation $\E[y_t | x_t]$
- Parameters $\param \in \Reals^{n \times m}$
- Function $\pi_\param : \Reals^n \to \Reals^m$, defined as
\[
\pi_\param(x_t) = \param^\top x_{t} = \sum_{i=1}^n \param_i x_{t,i}
\]
**** Probabilistic predictions: Modelling the conditional probability $\Pr[y_t | x_t]$
- $y_t = \E[y_t | x_t] + \epsilon_t$, with $\epsilon_t \in \Reals^m$ being zero-mean *noise*.
- Noise *covariance* matrix $\Sigma = \Var(\epsilon) =  \E[\epsilon_t \mid \epsilon_t \top]$ 



* Optimisation algorithms
** Gradient Descent
*** Gradient descent algorithm
**** Minimising a function
\[
\min_\param f(\param) \leq f(\param') \forall \param',
\qquad \param^* = \argmin_\param f(\param) \Rightarrow f(\param^*) = \min_\param f(\param)
\]
#+BEAMER: \pause
**** Gradient descent for minimisation
- Input $\param_0$
- For $n = 0, \ldots, N$:
- $\param_{n+1} = \param_n - \eta_n \nabla_\param f(\param_n)$
#+BEAMER: \pause
**** Step-size $\eta_n$
- $\eta_n$ fixed: for online learning
- $\eta_n = c/[c + n]$ for asymptotic convergence
- $\eta_n = \argmin_\eta f(\param_n + \eta \nabla_\param)$: Line search.

*** Gradient descent for squared error
**** The cost function
$L(\param, D) = \sum_{t=1}^T (y_t - \pi_\param(x_t))^2 =  \sum_{t=1}^T \epsilon_t^2$, with $\epsilon_t \defn y_t - \pi_\param(x_t)$.
#+BEAMER: \pause
**** Cost gradient
Using the chain rule of differentiation, $\nabla_\beta f(\epsilon) = \nabla_\epsilon f(\epsilon) \nabla_\beta \epsilon$.
\begin{align*}
\nabla_\param L(\param, D)
&= \nabla_\param \sum_{t=1}^T \epsilon_t^2
= \sum_{t=1}^T \nabla_\param \epsilon_t^2
= \sum_{t=1}^T \nabla_{\epsilon_t} \epsilon_t^2 \nabla_\beta \epsilon
\\
&
= \sum_{t=1}^T 2 \epsilon_t \nabla_\param[y_t -  \pi_\param(x_t)]
= \sum_{t=1}^T 2 [y_t - \pi_\param(x_t)] [- \nabla_\beta \pi_\param(x_t)]
\end{align*}
#+BEAMER: \pause
**** Parameter gradient for linear regression
Remember $\nabla_\beta f = (\partial / \partial_1 f, \ldots, \partial / \partial_n f)$
\[
\frac{\partial}{\partial \param_j} \pi_\param(x_t) 
=
\frac{\partial}{\partial \param_j} \sum_{i=1}^n \param_i x_{t,i}
=
\sum_{i=1}^n \frac{\partial}{\partial \param_j}  \param_i x_{t,i}
= x_{t,j}.
\]

*** Stochastic gradient descent algorithm
**** Note
 :PROPERTIES:
 :BEAMER_ENV: note
 :END:
For the general case, we got to do this.

**** When $f$ is an expectation
\[
f(\param) = \int_X dP(x) g(x, \param).
\]
**** Replacing the expectation with a sample:
\begin{align*}
\nabla f(\param)
&= \int_X dP(x) \nabla g(x, \param)\\
&\approx \frac{1}{K} \sum_{k=1}^K \nabla g(x^{(k)}, \param), && x^{(k)} \sim P.
\end{align*}

** Least-Squares
*** Analytical Least-Squares Solution
We need to solve the following equations for $A$:
\begin{equation*}
\begin{matrix}
y_1 &= x_1^\top \param\\
\cdots & \cdots\\
y_t &= x_t^\top \param\\
\cdots & \cdots\\
y_T &= x_T^\top \param
\end{matrix}
\end{equation*}
#+BEAMER: \pause
We can rewrite it in matrix form:
\begin{equation*}
\begin{pmatrix}
y_1\\
\vdots\\
y_t\\
\vdots\\
y_T
\end{pmatrix}
= 
\begin{pmatrix}
x_1^\top\\
\vdots\\
x_t^\top\\
\vdots\\
x_T^\top
\end{pmatrix}
\param
\end{equation*}
#+BEAMER: \pause
Resulting in 
\[
\by = X \param.
\]
#+BEAMER: \pause
How can we get $\param?$

*** Finding the $\param$
We now have a linear equation,
\[
\by = X \param.
\]
We want to solve for $\param$.
If $X$ had an inverse $X^{-1}$, we could obtain 
\[
X^{-1} \by = X^{-1}  X \param = I \param = \param.
\]
But $X^{-1}$ *does not exist*.

#+BEAMER: \pause
**** Least-squares solution
The *left-pseudo inverse* $\tilde{X}^{-1} \defn (X^\top X)^{-1} X^\top$ can be used to obtain
\[
\param = \tilde{X}^{-1} \by,
\]
This follows as:
\begin{align*}
\by &= X \param \\
 \tilde{X}^{-1} \by &=  \tilde{X}^{-1} X \param\\
 \tilde{X}^{-1} \by &= \underbrace{(X^\top X)^{-1}}_{A^{-1}} \underbrace{X^\top X}_{A} \param.
\end{align*}

*** Some matrix algebra reminders
**** The identity matrix $I \in \Reals^{n \times n}$
- For this matrix, $I_{i,i} = 1$ and $I_{i,j} = 0$ when $j \neq i$.
- $Ix = x$ and $IA = A$.
#+BEAMER: \pause
**** The inverse of a matrix $A \in \Reals^{n \times n}$
$A^{-1}$ is called the inverse of $A$ if
- $A A^{-1} = I$.
- or equivalently $A^{-1} A = I$.
#+BEAMER: \pause
**** The pseudo-inverse of a matrix $A \in \Reals^{n \times m}$
- $\tilde{A}^{-1}$ is called the *left pseudoinverse* of $A$ if $\tilde{A}^{-1} A = I$.
\[
\tilde{A}^{-1} = (A^\top A)^{-1} A^\top, \qquad n > m
\]
- $\tilde{A}^{-1}$ is called the *right pseudoinverse* of $A$ if $A \tilde{A}^{-1} = I$.
\[
\tilde{A}^{-1} =  A^\top (AA^\top)^{-1}, \qquad m > n
\]


* Regression libraries in Python
** sklearn
*** sklearn
**** Fitting a model to data
#+BEGIN_SRC python
  from sklearn.linear_model import LinearRegression
  model = LinearRegression().fit(X, Y) 
#+END_SRC


**** Getting predictions
We can get predictions for all inputs as an array
#+BEGIN_SRC python
Z = model.predict(X)
#+END_SRC


** statsmodels
*** Statsmodels
**** Fitting a model to data X, Y
#+BEGIN_SRC python
  import statsmodels.api as sm
  Xa = sm.add_constant(X) # adds a constant factor to the data
  model = sm.OLS(Y, Xa)
  results = model.fit() 
#+END_SRC
**** Getting predictions
The prediction is not just a point!
#+BEGIN_SRC python
  z = results.get_prediction(Xa[t])
  z.predicted_mean # This is E[y|x]
#+END_SRC


* Problems
** Interpretation Problem parameters
*** Pitfalls
- $\param_i$ tells us how much $y$ is correlated with $x_{t,i}$
- However, multiple correlations might be evident.
- Some features may be irrelevant
- The relationship may not be linear
- Correlation is not causation

*** Correlation is not causation
[[../fig/pirates-temp.png]]

** Exercises
*** Linear regression exercises
- Exercises 8, 13 from ISLP
- A variant of Ex. 13 but with Y generated independently of X.



