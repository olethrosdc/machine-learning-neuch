#+TITLE: Generative Modelling
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
#+OPTIONS:   H:3

* Graphical models
** Random variables and probabilities
*** Notational problems
In probability theory, we are dealing with functions $P$ on sets, called measures.
In statistics, we are dealing with random variables. While these two can be related, it is common to use shorthand notation in statistics. If in doubt, use the following conversion table.
**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
***** Probability theory
1. Sets $A \subset \Omega$ are *events*.
2. Probability measure $P$ on $\Omega$.
3. $P(A) \in [0,1]$ with $A \subset \Omega$
4. $P(A \cap C)$ the probability $\omega \in A \cap C$
5. $P(A | B)$ the probability of $A$ if $B$ is true.
6. Marginal distribution $P(A) = \sum_{i=1}^n \Pr(A \cap H_i)$
7. Bayes's theorem: $P(A \mid B) = P(A \cap B) / P(B)$.
**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
***** Statistics
1. Random variables $x : \Omega \to X$.
2. The measure $P$ is *implicit*.
3. $\Pr(x)$ for the distribution of $x$
4. $\Pr(x, y)$ the joint distribution of $x, y$.
5. $\Pr(x | y)$ the distribution of $x$ if $y$ is given.
6. Marginal distribution $\Pr(y) = \sum_{i \in x}  \Pr(x = i, y)$.
7. Bayes's theorem: $\Pr(x | y) = \Pr(x, y) / \Pr(y)$

*** Probability and statistics
**** Probability theory
Here, we talk a lot about *probability measures $P$* on some space $\Omega$.
These are functions on *sets*, so that 
- $P(\Omega) = 1$
- $P(A) \in [0,1]$ for any $A \subset \Omega$
- $P(A \cup B) = P(A) + P(B)$ if $A \cap B = \emptyset$.
*** Statistics
In statistics, we are interested in random variables $x, y, \ldots$
which are *functions*  $x : \Omega \to X$, t$y : \Omega \to Y$ etc.

- We write $\Pr(x)$ as a shorthand for /the probability distribution of $x$/.
- The distribution of $x$ arises in the following way:
  1. Sample $\omega$ from $P$
  2. Calculate $x(\omega)$.
- We can get a *probability measure $P_x$* by measuring how often $x$ falls in various sets $A \subset X$:
\[
P_x(A) \defn P(\{\omega : x(\omega) \in A\})
\]
Here, $\{\omega : x(\omega) \in A\}$ is literally the set of random outcomes $\omega$ for which $x(\omega)$ is in the set $A$. To avoid writing this cumbersome expression, we simply write
$\Pr(x)$ as a general shorthand. This allows us to also write
\[
\Pr(x \in A) = P_x(A).
\]

*** Joint distributions, multiple variables
If you thought things were messy, wait to see what happens when you have more than one variable! 
- $x : \omega \to X$, $y: \omega \to Y$.
- The measure $P_{x,y}$ has to be defined on the subsets of $X \times Y$.
\[
P_{x,y}(A) = P(\{\omega : (x(\omega), y(\omega)) \in A\}
\]

** Graphical model
*** Graphical models
#+ATTR_LATEX: :center 
\begin{center}
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB) to (xD);
      \draw[->] (xD) to (xi);
      \draw[->] (xB) to (xi);
    \end{tikzpicture}
\end{center}
**** Directed acyclic graph
A directed acyclic graph with 
- Nodes $x_1, \ldots, x_n$
- Edges $x_i \to x_j$.
so that there are no cycles in the graph.
**** Conditional independence from graphical models
- Each *node* of the model corresponds to a *random variable*
- The *parent* of a node are the *direct dependencies* of the random variable.

*** Model specification
\begin{center}
    \begin{tikzpicture}
      \node[RV] at (0,0) (x) {$x$};
      \node[RV] at (1,0) (y) {$y$};
      \node[RV] at (2,0) (z) {$z$};
      \draw[->] (x)--(y);
      \draw[->] (y)--(z);
    \end{tikzpicture}
\end{center}
- The graphical model tells us *what depends on what*.
- It does not tell us *how to generate data*.
- We need to specify the *functional relationship* between variables.
**** Col A                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
***** Statistics
    \begin{align*}
      \label{eq:factored-model}
      x &\sim f\\
      y \mid x = a &\sim g(a)\\
      z \mid y = b &\sim h(b)
    \end{align*}
**** Col B                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
***** Python
#+BEGIN_SRC python
  x = f()
  y = g(x)
  z = h(y)
#+END_SRC


*** Graphical models and factorisation
- Graphical models tell us what directly depends on what
- They allow us to simplify the *joint distribution* of the variables
- This is formed into a *product of factors*, one for each variable.
**** Example
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB)--(xD);
      \draw[->] (xD)--(xi);
    \end{tikzpicture}

    This graphical model implies the factorisation 
    \[
    \Pr(x_1, x_2, x_3) = 
    \Pr(x_3 \mid x_2, x_1) 
    \Pr(x_2 \mid x_1) 
    \Pr(x_1) = 
\Pr(x_3 \mid x_2) \Pr(x_2 \mid x_1) \Pr(x_1)
    \]
     Notice that for each factor $\Pr(x_i \mid x_j)$, $x_j$ is the *parent* of $x_i$.

      
*** Conditional independence
- Graphical models tell us what *directly* depends on what
- Consequently, they also specify *conditional independence*
**** Example
***** Col A                                                           :BMCOL:
      :PROPERTIES:
      :BEAMER_col: 0.5
      :END:
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB)--(xD);
      \draw[->] (xD)--(xi);
    \end{tikzpicture}
***** Col B                                                           :BMCOL:
      :PROPERTIES:
      :BEAMER_col: 0.5
      :END:
    \[
    \Pr(x_1, x_2, x_3) = \Pr(x_3 \mid x_2) \Pr(x_2 \mid x_1) \Pr(x_1)
    \]
**** Conditional independence :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
We say that variables $x, y$ are *conditionally* independent given $z$ and write $x \indep y \mid z$ if and only if
\[
\Pr(x, y \mid z) = \Pr(x \mid z) \Pr(y \mid z)
\]
- In the above example, it holds that $x_3 \indep x_1 \mid x_2$.


*** Smoking and lung cancer

      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$S$};
        \node[RV] at (1,1) (x2) {$C$};
        \node[RV] at (2,0) (x3) {$A$};
        \draw[->] (x1)--(x2);
        \draw[->] (x3)--(x2);
      \end{tikzpicture}
      
- Smoking and lung cancer graphical model.
- $S$: Smoking, $C$: cancer, $A$: asbestos exposure.

#+BEAMER: \pause
- In this graph, $A \indep S$, but $A \not \indep S \mid C$
#+BEAMER: \pause
**** XOR example
#+ATTR_BEAMER: :overlay <+->
- C = xor(S, A). 
- If we know C = 1, S = 1, what is $A$?
- C explains away
*** Time of arrival at work
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (1,1) (x2) {$T$};
        \node[RV] at (2,0) (x3) {$x_2$};
        \draw[->] (x2)--(x3);
        \draw[->] (x2)--(x1);
      \end{tikzpicture}
     
Time of arrival at work graphical model where $T$ is a traffic jam and $x_1$ is the time John arrives at the office and $x_2$ is the time Jane arrives at the office.

#+ATTR_BEAMER: :overlay <+->
- Even though $x_1, x_2$ are *not independent*, they become independent once you know $T$, i.e. $x_1 \indep x_2 \mid T$.
- Proof:
- $\Pr(x_1, x_2, T) = \Pr(x_2 \mid T) \Pr(x_1 \mid T) \Pr(T)$ from the graph.
- $\Pr(x_1, x_2, T) / \Pr(T) = \Pr(x_1, x_2 \mid T) = \Pr(x_2 \mid T) \Pr(x_1 \mid T)$. \qed

*** School admission
**** Example

***** Col A                                                           :BMCOL:
      :PROPERTIES:
      :BEAMER_col: 0.5
      :END:
|---------+------+--------|
| School  | Male | Female |
|---------+------+--------|
| A       |   62 |     82 |
| B       |   63 |     68 |
| C       |   37 |     34 |
| D       |   33 |     35 |
| E       |   28 |     24 |
| F       |    6 |      7 |
|---------+------+--------|
| Average |   50 | 27     |
***** Col B                                                           :BMCOL:
      :PROPERTIES:
      :BEAMER_col: 0.5
      :END:

- $z$: gender
- $s$: school applied to
- $a$: admission

**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:
        \begin{tikzpicture}
          \node[RV] at (0,0) (z) {$z$};
          \node[RV] at (1,1) (s) {$s$};
          \node[RV] at (2,0) (a) {$a$};
          \draw[->] (z)--(s);
          \draw[->] (z)--(s);
          \draw[->] (s)--(a);
        \end{tikzpicture}

Is admission independent of gender?
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

        \begin{tikzpicture}
          \node[RV] at (0,0) (z) {$z$};
          \node[RV] at (1,1) (s) {$s$};
          \node[RV] at (2,0) (a) {$a$};
          \draw[->] (z)--(s);
          \draw[->] (z)--(s);
          \draw[->] (s)--(a);
          \draw[->] (z)--(a);
        \end{tikzpicture}

	How about here?
** Exercises
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (b)--(c);
          \draw[->] (c)--(d);
        \end{tikzpicture}
#+BEAMER: \pause
\[
\Pr(a, b, c, d) = \Pr(d|c) \Pr(c|b) \Pr(b | a) \Pr(a)
\]
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (b)--(c);
          \draw[->] (c)--(d);
          \draw[->] (b)--(d);
        \end{tikzpicture}
#+BEAMER: \pause
\[
\Pr(a, b, c, d) = \Pr(d|b, c) \Pr(c|b) \Pr(b | a) \Pr(a)
\]
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (a)--(c);
        \end{tikzpicture}
#+BEAMER: \pause
\[
\Pr(a, b, c, d) = \Pr(d) \Pr(c|a) \Pr(b | a) \Pr(a)
\]
*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
	  \draw<2>[->] (a)--(b);
	  \draw<2>[->] (b)--(c);
	  \draw<2>[->] (b)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (c | b) P(d | b)
\]

*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
	  \draw<2>[->] (a)--(b);
	  \draw<2>[->] (c)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (d | c) P(c)
\]

*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
	  \draw<2>[->] (a)--(b);
	  \draw<2>[->] (a)--(c);
	  \draw<2>[->] (b)--(d);
	  \draw<2>[->] (c)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (c | a) P(d | b, c)
\]




*** Conditional independence (general)
- Consider variables $x_1, \ldots, x_n$.
- Let $B, D$ be subsets of $[n]$, and
- $\bx_B \defn (x_i)_{i \in B}$ be the variables with indices in $B$.
- $\bx_{-j} \defn (x_i)_{i \neq i}$ all the variables apart from $x_j$.
**** Conditional independence :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
  We say $x_i$ is *conditionally independent* of $\bx_B$ given $\bx_D$ and write 
  \[x_i \indep \bx_B \mid \bx_D\]
  if and only if:
  \[
  \Pr(x_i, \bx_B \mid \bx_D)
  =
  \Pr(x_i \mid \bx_D)
  \Pr(\bx_B \mid \bx_D).
  \]
- For this to hold in graphical model, $D$ must separate $i$ from $B$ in the graph.



*** More complex example
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$x_1$};
          \node[RV] at (0,2) (b) {$x_2$};
          \node[RV] at (2,0) (c) {$x_3$};
          \node[RV] at (2,2) (d) {$x_4$};
          \node[RV] at (4,0) (x) {$x_5$};
          \node[RV] at (4,2) (y) {$x_6$};
          \node[RV] at (6,1) (z) {$x_7$};
          \draw[->] (a)--(c);
          \draw[->] (c)--(d);
          \draw[->] (b)--(d);
          \draw[->] (d)--(y);
          \draw[->] (c)--(x);
          \draw[->] (y)--(z);
          \draw[->] (x)--(z);
        \end{tikzpicture}

In this example, we have:
\[
x_7 \indep x_1, x_2 \mid x_3, x_4
\]
and 
\[
x_7 \indep x_3 \mid x_4, x_5
\]

* Classification
** Classification: Generative modelling
   #+TOC: headlines [currentsection,hideothersubsections]
*** Generative modelling
**** General idea
- Data $(x_t,y_t)$.
- Need to model $P(y | x)$.
- Model the *complete* data distribution: $P(x | y)$, $P(x)$, $P(y)$.
- Calculate \(  P(y | x) = \frac{P(x | y) P(y)}{P(x)}. \)
**** Examples
- *Naive Bayes* classifier.
- *Gaussian mixture* model.
- Large language models.
**** Modelling the data distribution in classification
- Need to estimate the density $P(x | y)$ for each class $y$.
- Need to estimate $P(y)$.
*** The basic graphical model

**** A discriminative classification model
Here $P(y|x)$ is given directly.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \draw[->] (x) to (y);
\end{tikzpicture}

**** A generative classification model
Here $P(y | x) = P(x | y) P(y) / P(x)$.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \draw[->] (y) to (x);
\end{tikzpicture}
**** An unsupervised generative  model
Here we just have $P(x)$.
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
\end{tikzpicture}


*** Adding parameters to the graphical model
    
- We can also see the parameters of the distribution as (random) variables.
- We can put those random variables in the graphical model as well.
- Since the parameters are not observed, we denote them with dashed circles.
- They are a type of *latent* or *hidden* variable.

**** A Bernoulli RV
Here, $x | \theta \sim \Ber(\theta)$
\begin{tikzpicture}
\node[RV] at (2,0) (x) {$x$};
\node[RV,hidden] at (0,1) (mean) {$\theta$};
\draw[->] (mean) to (x);
\end{tikzpicture}

**** A normally distributed variable
Here $x  | \mu, \sigma \sim \Normal(\mu, \sigma^2)$
\begin{tikzpicture}
\node[RV] at (2,0) (x) {$x$};
\node[RV,hidden] at (0,1) (mean) {$\mu$};
\node[RV,hidden] at (1,1) (variance) {$\sigma$};
\draw[->] (mean) to (x);
\draw[->] (variance) to (x);
\end{tikzpicture}

*** Classification: Naive Bayes Classifier
- Data $(x,y)$
- $x \in X$
- $y \in Y \subset \mathbb{N}$, $N_i$: amount of data from class $i$.
#+BEAMER: \pause
**** Separately model each class
- Assume each class data comes from a different normal distribution
- $x | y = i \sim \Normal(\mu_i, \sigma_i I)$
- For each class, calculate
  - Empirical mean $\hat{\mu}_i = \sum_{t : y_t = i} x_t / N_i$
  - Empirical variance $\hat{\sigma}_i$.
#+BEAMER: \pause
**** Decision rule
Use Bayes's theorem:
\[
P(y | x) = P(x | y) P(y) / P(x),
\]
choosing the $y$ with largest posterior $P(y | x)$.
- $P(x | y = i) \propto \exp(- \|\hat{\mu}_i - x\|^2/\hat{\sigma}_i^2)$
*** Graphical model for the Naive Bayes Classifier
**** When $x \in \Reals$
Assume $k$ classes, then
- $\mu = (\mu_1, \ldots, \mu_k)$
- $\sigma = (\sigma_1, \ldots, \sigma_k)$
- \(\theta = (\theta_1, \ldots, \theta_k)\)
\begin{tikzpicture}
      \node[RV] at (2,0) (x) {$x$};
      \node[RV] at (0,0) (y) {$y$};
      \node[RV,hidden] at (2,1) (mean) {$\mu$};
      \node[RV,hidden] at (3,1) (variance) {$\sigma$};
      \node[RV,hidden] at (0,1) (choice) {$\theta$};
      \draw[->] (y) to (x);
      \draw[->] (mean) to (x);
      \draw[->] (variance) to (x);
      \draw[->] (choice) to (y);
\end{tikzpicture}
- $y \mid \theta \sim \Mult(\theta)$
- $x \mid y, \mu, \sigma \sim \Normal(\mu_y, \sigma^2_y)$
** Density estimation
*** Density estimation
The simplest type of generative model is just modelling the distribution of $x$. 
There are a number of models for this.

**** Parametric models
- Fixed histograms
- Gaussian Mixtures
**** Non-parametric models
- Variable-bin histograms
- Infinite Gaussian Mixture Model
- Kernel methods

*** Histograms
**** Fixed histogram
- Hyper-Parameters: number of bins
- Parameters: Number of points in each bin.
**** Variable histogram
- Hyper-parameters: Rule for constructing bins
- Generally $\sqrt{n}$ points in each bin.

*** Gaussian Mixture Model

**** Hyperparameters:
- Number of Gaussian $k$.
**** Parameters:
- Multinomial distribution $\vparam$ over Gaussians
- For each Gaussian $i$, center $\mu_i$, covariance matrix $\Sigma_i$.
**** Algorithms:
- Expectation Maximisation
- Gradient Ascent
- Variational Bayesian Inference (with appropriate prior)

*** Details of Gaussian mixture models
**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
***** Model. For each point $x_t$:
- $z_t \mid  \theta \sim \Mult(\theta_i)$, $\theta \in \Simplex^k$
- $x_t | z_t = i \sim \Normal(\mu_i, \Sigma_i)$.
- $\Mult(\theta)$ is *multinomial*
\[
\Pr(z_t = i \mid \theta) = \theta_i
\]
- $\Normal(\mu, \Sigma)$ is *multivariate Gaussian*
\[
p(x \mid \mu, \Sigma)
\propto \exp(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x-\mu))
\]
- The generating distribution is
\[
p(x | \theta, \mu, \Sigma) = \sum_{z \in [k]} p(x \mid \mu_z, \Sigma_z) P(z \mid \theta).
\]
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:


\begin{tikzpicture}
      \node[RV, hidden] at (2,0) (c) {$z$};
      \node[RV] at (0,0) (x) {$x$};
      \node[RV,hidden] at (0,-1) (mu) {$\mu$};
      \node[RV,hidden] at (0,1) (sigma) {$\Sigma$};
      \node[RV,hidden] at (2,1) (theta) {$\theta$};
      \draw[->] (c) to (x);
      \draw[->] (theta) to (c);
      \draw[->] (sigma) to (x);
      \draw[->] (mu) to (x);
\end{tikzpicture}

*** Applications of Gaussian mixture models
- Density estimation
- Clustering
- Used as part of a more complex model.

* Algorithms for latent variable models     

** Gradient algorithms
*** Gradient ascent
In the following we use $\theta$ for all the parameters of the Gaussian mixture model,
with $x = (x_1, \ldots, x_T)$ and $z = (z_1, \ldots, z_T)$
**** Objective function
One way to estimate $\theta$ is through maximising the likelihood
$L(\theta) = P(x | \theta)$
**** Marginalisation over latent variable
However, we need to marginalise over all values $z$
\[
L(\theta) = \sum_z P(z, x | \theta)
\]
For $T$ data points and $k$ different values of $z_t$, there are $k^T$ vectors $z$ to sum over.
**** Gradient ascent
If we can calculate the gradient of $L$, we can use gradient ascent to update our parameters:
\[
\theta^{(n+1)} = \theta^{(n)} + \alpha \nabla_\theta L(\theta).
\]
*** Gradient calculation
Here we use the *log trick*: $\nabla \ln f(\theta) = \nabla f(\theta) / f(\theta)$.
\begin{align}
\nabla_\theta L(\theta)
& = \sum_z \nabla_\theta P(z, x \mid \theta) 
\\
&= \sum_z  P(z, x \mid \theta) \nabla_\theta \ln P(z, x \mid \theta)
\\
&= \sum_z  P(x \mid z, \theta)P(z \mid \theta) \nabla_\theta \ln P(z, x \mid \theta)
\\
&\approx \frac{1}{m} \sum_{i=1}^m P(x \mid z^{(i)}, \theta) \nabla_\theta \ln P(z^{(i)}, x \mid \theta)
&&z^{(i)} \sim P(z  \mid \theta)
\end{align}
The final approximates the sum with the sample mean, sampling $z^{(i)}$ from the distribution. Hence, we can implement the following algorithm
- For $i = 1, \ldots, m$: $z^{(i)} \sim P(z  \mid \theta^{(n)})$
- $d^{(n)} = \frac{1}{m} \sum_{i=1}^m P(x \mid z^{(i)}, \theta) \nabla_\theta \ln P(z^{(i)}, x \mid \theta^{(n)})$
- $\theta^{(n+1)} = \theta^n + \alpha d^{(n)}$.
** Expectation maximisation
*** A lower bound on the likelihood
For any distribution $G(z)$, and specifically for
$G(z) = P(z | x, \theta^{(k)})$:
\begin{align*}
\ln P(x | \alert{\theta})
& = \sum_z G(z) \ln P(x | \alert{\theta})
 = \sum_z G(z) \ln [P(x, z | \alert{\theta}) / P(z | x, \alert{\theta})]
\\
& = \sum_z G(z) [\ln P(x, z | \alert{\theta}) - \ln P(z | x, \alert{\theta})]
\\
& = \sum_z G(z) \ln P(x, z | \alert{\theta}) - \sum_z G(z) \ln P(z | x, \alert{\theta})
\\
& = \sum_z P(z | x, \theta^{(k)}) \ln P(x, z | \alert{\theta}) - \sum_z P(z | x, \theta^{(k)}) \ln P(z | x, \alert{\theta})
\\
& \geq \sum_z P(z | x, \theta^{(k)}) \ln P(x, z | \alert{\theta}) - \sum_z P(z | x, \theta^{(k)}) \ln P(z | x, \theta^{(k)})
\\
& = Q(\alert{\theta} \mid \theta^{(k)}) +\mathbb{H}(z \mid x, \theta^{(k)}),
\end{align*}
where 
\[
\mathbb{H}(z \mid  x, \theta^{(k)})
= 
\sum_z  P(z \mid  x, \theta^{(k)}) \ln P(z \mid x, \theta^{(k)})
\]
is the entropy of $z$ for a fixed $x, \theta^{(k)}$. As this is not negative, $\ln P(x | \theta) \geq Q(\theta \mid \theta^{(k)})$.
*** Some information theory
Information theory notation can be a bit confusing. Sometimes we talk about random variables $\omega$, and sometimes about probability measures $P$. This is context-dependent.
**** Entropy
For a random variable $\omega$ under distribution $P$, we denote the entropy as
\[
 \mathbb{H}_P(\omega) \equiv \mathbb{H}(P) \equiv \mathbb{H}(\omega) =  \sum_{\omega \in \Omega} P(\omega) \ln P(\omega).
\]
**** KL Divergence
For two probabilities $P, Q$ over random outcomes in the same space $\Omega$, we define
\[
D_{KL}(P \|Q) = \sum_{\omega \in \Omega} P(\omega) \ln \frac{P(\omega)}{Q(\omega)}
\]
**** The Gibbs Inequality
$D_{KL}(P \|Q)  \geq 0$, or $\sum_x \ln P(x) P(x) \geq \sum_x \ln Q(x) P(x)$.
*** EM Algorithm (Dempster et al, 1977)
- Initial parameter $\vparam^{(0)}$, observed data $x$
- For $k=0, 1, \ldots$
-- Expectation step:
\[
Q(\alert{\vparam} \mid  \vparam^{(k)})
 \defn \E_{z \sim P(z | x, \vparam^{(k)})} [\ln P(x, z | \alert{\vparam}) ]
 = \sum_{z} [\ln P(x, z | \alert{\vparam})]  P(z  \mid x, \vparam^{(k)})
\]
-- Maximisation step:
\[
\vparam^{(k+1)} = \argmax_\vparam Q(\vparam, \vparam^{(k)}).
\]

See /Expectation-Maximization as lower bound maximization, Minka, 1998/

*** Minorise-Maximise
EM can be seen as a version of the minorise-maximise algorithm
- $f(\vparam)$: Target function to *maximise*
- $Q(\vparam | \vparam^{(k)})$: surrogate function
**** $Q$ Minorizes $f$
This means surrogate is always a lower bound so that
\[
f(\vparam) \geq Q(\vparam | \vparam^{(k)}),
\qquad
f(\vparam^{(k)}) \geq Q(\vparam^{(k)} | \vparam^{(k)}),
\]

**** Algorithm
- Calculate: $Q(\vparam | \vparam^{(k)})$
- Optimise: $\vparam^{(k+1)} = \argmax_\vparam Q(\vparam | \vparam^{(k)})$.



* Exercises
** Density estimation
*** GMM versus histogram
- Generate some data $x$ from an arbitrary distribution in $\Reals$.
- Fit the data with a histogram for varying numbers of bins
- Fit a GMM with varying numbers of Gaussians
- What is the best fit? How can you measure it?

** Classification
*** GMM Classifier :exercise:
**** Base class: sklearn GaussianMixtureModel
- /fit()/ only works for Density Estimaiton
- /predict()/ only predicts cluster labels
**** Problem
- Create a GMMClassifier class
- /fit()/ should take X, y, arguments
- /predict()/ should predict class labels
- Hint: Use /predict_proba()/ and multiple GMM models


