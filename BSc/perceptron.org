#+TITLE: The perceptron algorithm
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{pgfplots}
#+LaTeX_HEADER: \usetikzlibrary{datavisualization}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}


* The Perceptron
** Introduction
*** Guessing gender from height
- Feature space $\CX \subset \Reals$: e.g. height
- Label space $\CY = \{-1, 1\}$: e.g. gender
- Can we find some $\param_1 \in \Reals$ and a direction  $\param_0 \in \{-1, +1\}$ so as to separate the genders?
**** Online learning: At time $t$
- We choose a separator $\param^t_0, \param_1^t$
- We observe a new datapoint $x_t, y_t$
- We make a mistake at time $t$ if:
\[
\param^t x_t - \param_0^t \leq 0.
\]
- If we stop making mistakes, then we are classifying everything perfectly.

**** Can you find a threshold that makes a small number of mistakes?
[[./src/Perceptron/perceptron_simple.py]]

*** Non-separable classes
[[./fig/histogram_heights.png]]
- In general, we cannot perfectly classify everything
- But we can estimate $\Pr(y \mid x)$ \ldots more on this later.

*** A more complex example
- Feature space $\CX \subset \Reals^n$: e.g. height and weight for $n=2$
- Label space $\CY = \{-1, 1\}$: e.g. gender
- Can we find some line so as to separate the genders?
-[[./src/Perceptron/show_class_data_labels.py]]
- Is there an algorithm for doing so?

*** A linear classifier
**** The separating hyperplane
We now have parameters $\param_0 \in \Reals$ and $\param \in \Reals^n$
defining a *hyperplane* $f(x) = 0$ in $\Reals^n$
\[
f(x) = \param_0 + \param^\top x
 = \param_0 + \sum_{i=1}^n \param_i x_i.
\]
#+BEAMER: \pause
If we augment $x$ with an additional component $x_0 = 1$,  we can write
\[
f(x) = \param^\top x
 =  \sum_{i=0}^n \param_i x_i.
\]
#+BEAMER: \pause
**** The classifier
The *perceptron decision rule* is $\pi(x) = \textrm{sign}(f(x))$
- If $f(x) > 0$, we assign class +1
- If $f(x) < 0$, we assign class -1
*** Hyperplanes in 2 dimensions (lines)

\begin{tikzpicture}[domain=-1:3]
   \draw[dotted, color=gray] (-1.1,-3.1) grid (5.1,4.1);
   \draw[->] (0,0) -- (4,0) node[right] {$x_1$};
   \draw[->] (0,0) -- (0,4) node[above] {$x_2$};
   \draw[thick, color=blue]   plot (\x, {0 + \x * 1/2})  node[right] {$\param = (0, 1, 2)$};
   \draw[--,thick, color=magenta]   plot (\x, {1 + \x * 1/1})  node[right] {$\param = (1, 1, 1)$};
   \draw[-.,thick, color=red]   plot (\x, {1 - \x * 1/1})  node[right] {$\param = (1, 1, -1)$};
\end{tikzpicture}

These lines are the solution to $f(x) = 0$
** The algorithm
*** The Perceptron
**** Pitts and McCulloch                                              :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
#+attr_html: :width 60x
#+attr_latex: :width 60px
#+NAME: piits
#+CAPTION: Pitts
[[../fig/pitts.jpg]]
#+attr_html: :width 60px
#+attr_latex: :width 60px
#+CAPTION: McCulloch
[[../fig/McCulloch.jpeg]]

**** Rosenblatt and the Perceptron                                    :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
#+attr_html: :width 60px
#+attr_latex: :width 60px
#+CAPTION: Rosenblatt
[[../fig/rosenblatt.jpeg]]

#+attr_html: :width 60px
#+attr_latex: :width 60px
#+CAPTION: Perceptron Mark I
[[../fig/Mark_I_perceptron.jpeg]]

*** The perceptron algorithm
**** Input
- Feature space $X \subset \Reals^n$.
- Label space $Y = \{-1, 1\}$.
- Data $(x_t, y_t)$, $t \in [T]$,  with $x_t \in X, y_t \in Y$.
**** Algorithm
+ $\param^0 \sim \Normal^n(0, I)$. % Initialise parameters
+ For $t = 1, \ldots, T$
  - $a_t = \sgn(\param^t \cdot x_t)$. % Classify example
  - If $a_t \neq y_t$
	- $\param^{t} = \param^{t-1} + y_t x_t$ % Move hyperplane
  - Else
	- $\param^{t} = \param^{t-1}$ % Do nothing for correct examples
  - EndIf
+ Return $\param^{T}$
	 

*** Perceptron examples
**** Example 1: One-dimensional data
- Done on the board
- Shows how the algorithm works.
- Demonstrates the idea of a margin

**** Example 2: Two-dimensional data
- See [[file:src/NeuralNetworks/perceptron.py][in-class programming exercise]]
*** Margins and the perceptron theorem
#+attr_html: :width 120px
#+attr_latex: :width 120px
[[./fig/margin.pdf]]
- The *hyperplane* $\param^*$ separates the examples
- The *margin* $\rho$ is the minimum distance $\rho$ between $\param^*$ and any point.
**** Perceptron theorem :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
 The number of mistakes is bounded by $\rho^{-2}$, where $\|x_t\|\leq
 1$, $\rho \leq y_t (x_t^\top \param^*)$ for some *margin* $\rho$ and
 *hyperplane* $\param^*$ with $\|\param^*\|=1$.
*** Simple proof
#+ATTR_BEAMER: :overlay +-
- Scale data: $\|x\| \leq 1$
- Separating plane: $y_t(x_t \cdot \param^*) \geq \rho \forall t$, $\|\param^*\| = 1$.
- When we make an update: $y_t(x_t \cdot \param^t) \leq 0$.
- At each mistake, $\param^t \cdot \param^*$ grows by *at least $\rho$*.
#+BEAMER: \pause
\[
\param^{t+1} \cdot \param^* = (\param^t + yx_t) \cdot \param^* = \param^t \cdot \param^* + y(x_t \cdot \param^*) \geq \param^t \cdot \param^* + \rho
\]
#+BEAMER: \pause
- At each mistake, $\param \cdot \param$ grows by *at most 1.*
\[
\param^{t+1} \cdot \param^{t+1}
 = (\param^t + yx_t) \cdot (\param^t + yx_t)
= \param^t \cdot \param^t + 2y(\param^t \cdot x_t) + y^2(x_t \cdot x_t) \leq \param^t \cdot \param^t + 1
\]
#+BEAMER: \pause
**** Putting it together
After $M$ mistakes:
#+ATTR_BEAMER: :overlay +-
- $\param^t \cdot \param^* \geq M \rho$
- $\param^t \cdot \param^t \leq M$
- So $M \rho \leq \param^t \cdot \param^* \leq \|\param^t\| = \sqrt{\param^t \cdot \param^t} \leq \sqrt{M}$.
- Thus, $M \leq \rho^{-2}$.




*** Promise of the perceptron   
[[../fig/nyt_perceptron.png]]
*** Promise versus reality
**** Focus on classification
- Rosenblatt only consider classification problems
- Many problems in learning and AI are not simply classification problems
- Classification requires labels. These are not always easily available.
**** Separable representation assumption
- Rosenblatt assumed that there was a representation available that would allow us to distinguish classes.
- However, it is not clear /a priori/ how to obtain such a data representation from the data. Progress followed roughly these steps:
  - Hand-crafted features
  - Random features
  -  Multi-layer perceptrons, hand-crafted architectures, and backpropagation
  - Attention mechanisms
* Gradient methods
** Gradients for optimisation
*** The gradient descent method: one dimension
- Function to minimise $f : \Reals \to \Reals$.
- Derivative $\frac{d}{d \param} f(\param)$
**** Gradient descent algorithm
- Input: initial value $\param^0$, *learning rate* schedule $\alpha_t$
- For $t=1, \ldots, T$
  - $\param^{t+1} = \param^t - \alpha_t \frac{d}{d \param} f(\param^t)$
- Return $\param^T$

**** Properties
- If $\sum_t \alpha_t = \infty$ and $\sum_t \alpha_t^2 < \infty$, it finds a local minimum $\param^T$, i.e. there is $\epsilon > 0$ so that
\[
f(\param^T) < f(\param), \forall \param: \|\param^T - \param\| < \epsilon.
\]
*** Gradient methods for expected value :example:
**** Estimate the expected value
$x_t \sim P$ with $\E_P[x_t] = \mu$.
#+BEAMER: \pause
**** Objective: mean squared error
Here $\ell(x, \param) = (x - \param)^2$.
\[
\min_\param \E_P[(x_t - \param)^2].
\]
#+BEAMER: \pause
**** Exact gradient update
If we know $P$, then we can calculate
\begin{align}
\param^{t+1} &= \param^t - \alpha_t \frac{d}{d\param} \E_P[(x - \param^t)^2]\\
\frac{d}{d\param} \E_P[(x - \param^t)^2] &= 2 \E_P[x] - \param^t
\end{align}

*** Gradient for mean estimation :example:
- Let us show this in detail
\begin{align*}
 \frac{d}{d\param} \E_P [(x - \param)^2] 
&= \int_{-\infty}^\infty dP(x) \frac{d}{d\param} (x - \param)^2
\\
&=  \int_{-\infty}^\infty dP(x) 2(x - \param)
\\
&=  2 \E_P[x] - 2\param.
\end{align*}
- If we set the derivative to zero, then we find the optimal solution:
\[
\param^* = \E_P[x]
\]
- How can we do this if we only have data $x_t \sim P$?
*** Mean-squared error cost function
\begin{tikzpicture}[domain=-1:2, range=-1:2]
   \draw[dotted, color=gray] (-1.1,-2.1) grid (3.1,4.1);
   \draw[->] (0,0) -- (2,0) node[right] {$\param$};
   \draw[->] (0,0) -- (0,4) node[above] {$\ell$};
   \draw[color=red] plot (\x, {(\x-1)^2})  node[right] {$\mu = 1$};
   \draw[color=blue] plot (\x, {(\x)^2})  node[right] {$\mu = 0$};
\end{tikzpicture}
Here we see a plot of $\ell(\mu, \param) = (\param - \mu)^2$.
*** Stochastic gradient for mean estimation
**** Sampling :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
For any bounded random variable $f$, 
\[
\E_P[f] = \int_{X} dP(x) f(x)
 = 
\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T f(x_t)
 = 
\E_P \left[\frac{1}{T} \sum_{t=1}^T f(x_t)\right]
, \qquad x_t \sim P
\]
**** Sampling :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
- If we sample $x$ we approximate the gradient:
\begin{align*}
 \frac{d}{d\param} \E_P [(x - \param)^2] 
= \int_{-\infty}^\infty \!\!\!\! dP(x) \frac{d}{d\param} (x - \param)^2
\approx \frac{1}{T} \sum_{t=1}^T \frac{d}{d\param} (x_t - \param)^2
= \frac{1}{T} \sum_{t=1}^T 2(x_t - \param)
\end{align*}
#+BEAMER: \pause
- If we update $\param$ after each new sample $x_t$, we obtain:
\[
\param^{t+1} = \param^t + 2 \alpha_t (x_t - \param^t)
\]

*** The gradient method
- Function to minimise $f : \Reals^n \to \Reals$.
- Derivative $\nabla_\param f(\param)  = \left(\frac{\partial f(\param)}{\partial \param_1}, \ldots, \frac{\partial f(\param)}{\partial \param_n}\right)$,
 where $\frac{\partial f}{\partial \param_n}$ denotes the *partial* derivative, i.e. varying one argument and keeping the others fixed.
**** Gradient descent algorithm
- Input: initial value $\param^0$, learning rate schedule $\alpha_t$
- For $t=1, \ldots, T$
  - $\param^{t+1} = \param^t - \alpha_t \nabla_\param f(\param^t)$
- Return $\param^T$

**** Properties
- If $\sum_t \alpha_t = \infty$ and $\sum_t \alpha_t^2 < \infty$, it finds a local minimum $\param^T$, i.e. there is $\epsilon > 0$ so that
\[
f(\param^T) < f(\param), \forall \param: \|\param^T - \param\| < \epsilon.
\]
*** Stochastic gradient method
This is the same as the gradient method, but with added noise:
- $\param^{t+1} = \param^t - \alpha_t [\nabla_\param f(\param^t) + \omega_t]$
- $\E[\omega_t] = 0$ is sufficient for convergence.
#+BEAMER: \pause
**** When the cost is an expectation                     :B_example:
	 :PROPERTIES:
	 :BEAMER_env: example
	 :END:
In machine learning, the cost is frequently an expectation of some function $\ell$, 
\[
f(\param) = \int_X dP(x) \ell(x, \param)
\]
This can be approximated with a sample
\[
f(\param) \approx \frac{1}{T} \sum_t \ell(x_t, \param)
\]
The same holds for the gradient:
\[
\nabla_\param f(\param) = \int_X dP(x) \nabla_\param \ell(x, \param)
\approx \frac{1}{T} \sum_t \nabla_\param \ell(x_t, \param)
\]

** The perceptron as a gradient algorithm
*** Perceptron algorithm as gradient descent
**** Target error function
\[
\E_{\alert{P}}^\param[\ell] = \int_{\CX} d\alert{P}(x) \sum_y \alert{P}(y|x) \ell(x, y, \param)
\]
Minimises the error on the true distribution.
#+BEAMER: \pause
**** Empirical error function
\[
\E_{\alert{D}}^\param[\ell]= \frac{1}{T} \sum_{t=1}^T \ell(x_t, y_t, \param),
\qquad\alert{D} = (x_t, y_t)_{t=1}^T, \quad x_t, y_t \sim P.
\]
Minimises the error on the empirical distribution.
*** Cost functions and the chain rule
**** Perceptron cost function
The cost of each example
\begin{align}
\ell(x,y, \param) 
&= \overbrace{\ind{y(x^\top \param) < 0}}^{\textrm{misclassified?}} \overbrace{[ - y (x^\top \param)]}^{\textrm{margin of error}}
\end{align}
where the *indicator function $\ind{A}$* is  1 when $A$ is true and $0$ otherwise.
\begin{center}
\begin{tikzpicture}[domain=-2:2, samples=200,range=-1:2]
   \draw[dotted, color=gray] (-2.1,-2.1) grid (3.1,3.1);
   \draw[->] (0,0) -- (2,0) node[right] {$f(x)$};
   \draw[->] (0,0) -- (0,4) node[above] {$\ell$};
   \draw[thick, color=red] plot (\x, {max(0, \x)}) node [right] {perceptron cost};
   \draw[dashed, thick, color=blue] plot (\x, {\x >= 0)}) node [right] {classification cost};
\end{tikzpicture}
\end{center}

Here we see a plot of $\ell(\mu, \param) = (\param - \mu)^2$.
*** Derivative of the perceptron cost function
The total cost over the data is defined as $L(D, \param) = \sum_{(x, y) \in D} \ell(x, y, \param)$.
Taking the derivative, we have
\[
\nabla_\param L(D, \param) = \nabla_\param \sum_{(x, y) \in D} \ell(x, y, \param)
  = \sum_{(x, y) \in D} \nabla_\param \ell(x, y, \param)
\]

**** Reminder: The chain rule
Let $z = g(y)$, $y = f(x)$ so that $z= g(f(x))$. Then $\frac{dz}{dx} = \frac{dz}{d\alert{y}}\frac{d\alert{y}}{dx}$

#+BEAMER: \pause
**** Applying the chain rule to calculate the gradient
#+ATTR_BEAMER: :overlay <+->
- Set $z =  \ell(x,y, \param)$.
- $\nabla_\param \ell(x,y, \param) = - (\frac{d}{dz} \ind{z < 0} z) \nabla_\param z$.
- $\nabla_\param \ell(x,y, \param) = - \ind{y(x^\top \param) < 0} \nabla_\param [y(x^\top \param)]$.
- $\frac{\partial}{\partial{\param_i}} [y(x_t^\top \param)] = y x_{t,i}$ (gradient of Perceptron's output)
- Gradient update: $\param^{t+1} = \param^t - \nabla_\param \ell(x,y, \param) = \param^t + y x_{t}$
#+BEAMER: \pause
The classification error cost function is *not* differentiable :(
*** Margins and confidences
#+ATTR_BEAMER: :overlay <+->
We can think of the output of the network as a measure of confidence
#+attr_html: :width 100px
#+attr_latex: :width 100px
[[./fig/margin.pdf]]
#+BEAMER: \pause
By applying the *logit* function, we can bound a real number $x$ to $[0,1]$:
\[
f(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}
\]
*** Logistic regression
**** Output as a measure of confidence, given the parameter $\param$
\[
P_\param(y = 1| x) = \frac{1}{1 + \exp(- x_t^\top \param)}
\]
The original output $x_t^\top \param$ is now passed through the logit function.
#+BEAMER: \pause
**** Negative Log likelihood
#+ATTR_BEAMER: :overlay <+->
$\ell(x_t, y_t, \param) = - \ln P_\param( y_t | x_t) = \ln(1 + \exp(- y_t x_t^\top \param))$
\begin{align*}
\nabla_\param \ell(x_t, y_t, \param) 
&= \frac{1}{1 + \exp(- y x_t^\top \param)} \nabla_\param[1 + \exp(-y x_t^\top \param)]
\\
&= \frac{1}{1 + \exp(- y x_t^\top \param)} \exp(-y x_t^\top \param) [\nabla_\param (-y_t x_t^\top \param)]
\\
&= - \frac{1}{1 + \exp(x_t^\top \param)} (x_{t,i})_{i=1}^ne
\end{align*}
- $\E_P(\ell) = \int_X dP(x) \sum_{y \in Y} P(y|x) P_\param(y_t + x_t)$
* Lab and Assignment

**** The Perceptron and Gradients
[[./src/Perceptron/Perceptron_gd.ipynb]]
- Perceptron implemenation to fill in
- Gradient descent implementation
- Experiment on the learning rate with sklearn


