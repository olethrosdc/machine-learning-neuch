{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1716436-295f-4149-af37-790849343870",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "When we begin a data analysis, we want to create a standard methodlogical pipeline.\n",
    "The pipeline has the following stages:\n",
    "1. Data collection\n",
    "2. Data processing\n",
    "3. Fitting models\n",
    "4. Testing the quality of the fit.\n",
    "5. Drawing conclusions from the fit.\n",
    "\n",
    "If we pack together all of these steps into a single pipeline, then we ensure the reproducibility of our results.\n",
    "Below, I have a number of examples. Some of them are based on the standard /Pipeline/ offered by sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc647b-ffcc-4385-866b-afc15c33e57d",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "Here the data is loaded and the most basic operations are performed. In this case, we can remove some columns that we /a priori/ know are useless. In a simulation setting, however, data collection is going to be done by sampling from a distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b948a6e2-516c-44fd-848a-a1e570e2450e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Intro to ML Class.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIntro to ML Class.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPseudonym\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m data\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Intro to ML Class.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Intro to ML Class.csv\")\n",
    "data = data.drop(['Pseudonym'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24524d-1444-4cfc-ba38-edf30ef7fdc7",
   "metadata": {},
   "source": [
    "## Step 2 and 3. Data preprocessing and fitting\n",
    "\n",
    "sklearn allows us to create a single pipeline for all the preprocessing we want to do. This is very useful if we already know what kind of processing we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ada8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "classification_pipeline = Pipeline([('scaler', StandardScaler()), ('perceptron', Perceptron())])\n",
    "classification_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac018e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "X = data.copy().drop(['Gender'], axis=1)\n",
    "y = data['Gender']\n",
    "classification_pipeline.fit(X, y)\n",
    "classification_pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41a859-e271-4dbc-a8a9-6759c2d14275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c978ce-a15b-40ca-a223-38d0f5c20b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [ \"Height\", \"Age\", \"Sporthours/week\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [\"Gender\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a7ba1-9e64-4d6f-afb0-bc572f768df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Intro to ML Class.csv\")\n",
    "data = data.drop(['Pseudonym'], axis=1)\n",
    "X = data.copy().drop([\"Weight\"], axis=1)\n",
    "y = data[\"Weight\"].copy()\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor\", LinearRegression())]\n",
    ")\n",
    "clf.fit(X, y)\n",
    "print(clf.predict(X) - y) # show the prediction error\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66187d4-579c-4a6f-ac29-ae099f3a1f72",
   "metadata": {},
   "source": [
    "# From simple pipelines to simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e369e8-185b-440c-b4a2-f9d85f1c2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28511ae-1652-4dee-b65d-9177b4a33ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We can make a number of different simulations, one for each case\n",
    "def SimpleClassDataGenerator(n_samples, m_height=175, f_height=165, m_weight=0.5, f_weight=0.5):\n",
    "    features = [\"Height\", \"Weight\", \"Gender\", \"Age\", \"Sporthours/week\", \"Location\"]\n",
    "    n_features = len(features)\n",
    "    X = np.zeros([n_samples, n_features])\n",
    "    df = pd.DataFrame(X, columns=features)\n",
    "    for t in range(n_samples):\n",
    "        # Assume no relation between gender, age, location and sports\n",
    "        # However this assumption might be false\n",
    "        df.at[t, 'Gender']= np.random.choice(2)\n",
    "        age = 18+np.random.negative_binomial(5,0.3)\n",
    "        df.at[t, 'Age']= age\n",
    "        df.at[t, 'Location']= np.random.choice([0,1,2,3], p=[0.2, 0.3, 0.45, 0.05])\n",
    "        df.at[t, 'Sporthours/week'] = np.random.negative_binomial(4,0.5)\n",
    "        sport_weight =  age / (age  + df.at[t, 'Sporthours/week']) # older people lose weight less effectively?\n",
    "        if (df.at[t,'Gender']== 1):            \n",
    "            df.at[t,'Height'] = np.random.gamma(m_height,1)\n",
    "            df.at[t,'Weight'] = (df.at[t, 'Height'] + np.random.gamma(20,1))*m_weight*sport_weight\n",
    "            \n",
    "        else:\n",
    "            df.at[t,'Height'] = np.random.gamma(f_height,1)\n",
    "            df.at[t,'Weight'] = (df.at[t, 'Height'] + np.random.gamma(20,1))*f_weight*sport_weight\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721a155-9e46-4644-8916-2310bcc66fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65947ebe-c9c9-416c-b27e-47f6ec6e33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def my_pipeline(data):\n",
    "    # get data\n",
    "    X = data.copy().drop([\"Weight\"], axis=1)\n",
    "    y = data[\"Weight\"].copy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "    # construct data processing pipeline\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    mse = np.mean((y_pred - y_test)**2)\n",
    "    return mse, clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c7556-79e0-47b2-914e-75a9e19a3f8f",
   "metadata": {},
   "source": [
    "# Experiment design\n",
    "\n",
    "We can perform multiple experiments to take into account the variability of the data.\n",
    "We can start by seing see how well we can predict the weight from measurements with different amounts of data.\n",
    "By altering the simulator parameters, we can see how well our model is able to capture the dependencies between different variables. This can inform our experiment design:\n",
    "- How much data do we need to collect?\n",
    "- When can we expect to have a reasonable result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4c71e-da76-42be-88f2-3978ce6fbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = [10,20,50,100,200,500,1000,2000,5000,10000]\n",
    "n_experiments=10\n",
    "mse = np.zeros([n_experiments, len(data_sizes)])\n",
    "c_height = np.zeros([n_experiments, len(data_sizes)])\n",
    "c_gender = np.zeros([n_experiments, len(data_sizes)])\n",
    "\n",
    "for t in range(n_experiments):\n",
    "    k = 0\n",
    "    for n_data in data_sizes:\n",
    "        mse[t, k], coeffs   = my_pipeline(SimpleClassDataGenerator(n_data))\n",
    "        c_height[t, k] = coeffs[0]\n",
    "        c_gender[t, k] = coeffs[1]\n",
    "        k+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6530f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821caff-598a-4ef1-b9bd-5799a063050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from this plot, we need at least 100 measurements \n",
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(data_sizes, mse.transpose(), '--')\n",
    "plt.loglog(data_sizes, np.mean(mse, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869ed9d-61b5-4632-9a14-927c4fab47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from this plot, we need at least 100 measurements \n",
    "plt.loglog(data_sizes, c_height.transpose(), '--')\n",
    "plt.loglog(data_sizes, np.mean(c_height, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc8cf0-eff4-408b-a50e-e3f33c2e302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from this plot, we need at least 100 measurements \n",
    "plt.semilogx(data_sizes, c_gender.transpose(), '--')\n",
    "plt.semilogx(data_sizes, np.mean(c_gender, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79841bbb-bac5-4366-b3b7-4a89156503bc",
   "metadata": {},
   "source": [
    "Let's now repeat the experiment for classification...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb820e-6f9a-4c2f-84f6-afe2b29e56ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
