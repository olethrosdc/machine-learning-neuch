#+TITLE: Latent variable models
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
#+OPTIONS:   H:3

* Latent variable models
** Introduction
*** Types of latent variables
- Mixture model
- Latent state in a dynamical system
- Personal information
- Objects in an image  
** Gaussian mixture models
*** Gaussian Mixture Models

In this model, data is generated from one of $k$ Gaussian
distributions with unknown mean and variance.

**** Graphical model                                                  :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5\textwidth
:END:
#+begin_center
\begin{tikzpicture}
\node[RV] at (0,1) (x) {$x_t$};
\node[RV,hidden] at (0,0) (c) {$c_{t}$};
\node[RV,hidden] at (0,-1) (cat) {$\vparam$};
\node[RV,hidden] at (1,-1) (mean) {$\vectorsym{\mu}$};
\node[RV,hidden] at (2,-1) (var) {$\vectorsym{\Sigma}$};
\draw[->] (cat) to (c);
\draw[->] (c) to (x);
\draw[->] (mean) to (x);
\draw[->] (var) to (x);
\end{tikzpicture}
#+end_center
**** Basic model :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5\textwidth
:END:

\begin{align*}
c_t \mid \vparam &\sim \Mult(\vparam),\\
\bx_t \mid \vectorsym{\mu}_i, \matrixsym{\Sigma}_i, c_t = i & \sim \Normal(\vectorsym{\mu}_i, \matrixsym{\Sigma}_i).
\end{align*}

**** Variables
- $c_t \in [k]$ specifies which distribution generated the t-th example.
- $\vparam \in \Simplex^k$ is the parameter of the distribution generating $c_t$
- $x_t \in \Reals^n$ is the t-th example
- $\vectorsym{\mu}_i \in \Reals^n$ - is the mean of the i-th Gaussian
- $\matrixsym{\Sigma}_i \succcurlyeq 0 \in \Reals^{n \times n}$ is the covariance of the i-th Gaussian
*** Maximum Likelihood Inference for Gaussian Mixture Models
**** Maximum likelihood
- Use the EM algorithm to solve
  \[
  \max_{\vparam, \vectorsym{\mu}, \vectorsym{\Sigma}} P(x_t \mid \vparam, \vectorsym{\mu}, \vectorsym{\Sigma})
  \]
  by adding the latent variable into the mix:
  \[
  \max_{\vparam, c_i, \vectorsym{\mu}, \vectorsym{\Sigma}} P(x_1, \ldots, x_t \mid c_1, \ldots, c_t, \vparam, \vectorsym{\mu}, \vectorsym{\Sigma})
  \]
*** Bayesian Inference for Gaussian Mixture Models
**** Graphical model                                                  :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4\textwidth
:END:
#+BEGIN_CENTER
\begin{tikzpicture}
\node[RV] at (0,1) (x) {$x_t$};
\node[RV,hidden] at (0,0) (c) {$c_t$};
\node[RV,hidden] at (0,-1) (cat) {$\vparam$};
\node[RV,hidden] at (1,-1) (mean) {$\vectorsym{\mu}$};
\node[RV,hidden] at (2,-1) (var) {$\vectorsym{\Sigma}$};
\draw[->] (cat) to (c);
\draw[->] (c) to (x);
\draw[->] (mean) to (x);
\draw[->] (var) to (x);
\node[RV] at (0,-2) (aprior) {$\vectorsym{\alpha}$};
\node[RV] at (1,-2) (gprior) {$\vectorsym{v}$};
\node[RV] at (2,-2) (sprior) {$\vectorsym{W}$};
\draw[->] (aprior) to (cat);
\draw[->] (gprior) to (mean);
\draw[->] (sprior) to (var);
\end{tikzpicture}
#+END_CENTER
**** Bayesian model :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6\textwidth
:END:
#+BEGIN_CENTER
\begin{align}
\vectorsym{\mu_i} \mid \vectorsym{v} &\sim \Normal(\vectorsym{v}, \vectorsym{I}),\\
\vectorsym{\Sigma_i} \mid \vectorsym{W} &\sim \Wish(\vectorsym{W})\\
\vparam \mid \vectorsym{\alpha} &\sim \Dir(\vectorsym{\alpha}),\\
c_t \mid \vparam &\sim \Mult(\vparam),\\
\bx_t \mid \vectorsym{\mu}, \matrixsym{\Sigma}, c_t = i & \sim \Normal(\vectorsym{\mu}_i, \matrixsym{\Sigma}_i).
\end{align}
#+END_CENTER

* Sequential models
** Markov models
*** Markov model
#+BEGIN_CENTER
\begin{tikzpicture}
\node[RV] at (-1,1) (x0) {$x_{t-1}$};
\node[RV] at (0,1) (x1) {$x_t$};
\node[RV] at (1,1) (x2) {$x_{t+1}$};
\draw[->] (x0) to (x1);
\draw[->] (x1) to (x2);
\end{tikzpicture}
#+END_CENTER

**** Markov property
The next observation is independent of the previous ones
\[
\Pr(x_{t+1} \mid x_t, x_{t-1}, \ldots, x_1)
= \Pr(x_{t+1} \mid x_t).
\]

**** Model for discrete $x$
When $x \in \{1, \ldots, k\}$, we can set
\[
\Pr(x_{t+1} = j \mid x_t = i) = \theta_{i,j},
\]
requiring $k^2$ parameters.
**** n-order Models
If we have longer-term dependencies, we can use n-order models:
\[
\Pr(x_{t+1} \mid x_t, x_{t-1}, \ldots, x_1)
= \Pr(x_{t+1} \mid x_t, \ldots, x_{t-n+1}).
\]

*** Continuous-variable Markov models

**** Window
It is convenient to refer to the last $n$ observations as the *window*:
\[ 
x_{t-n}^{t-1} = x_{t-n} , \ldots, x_{t-1}
\]

**** Linear \(n\)-order Markov model (auto-regressive model)
This is a linear model with Gaussian noise:
\begin{align}
x_t \mid x_{t-n}^{t-1}, \vparam, \sigma &\sim \Normal(\vparam^\top x_{t-n}^{t-1}, \sigma^2)
\\
\E [x_{t} \mid x_{t-1}, \ldots, x_{t-n}, \vparam]
&= \sum_{i=1}^n \param_i x_{t-i}  = \vparam^\top x_{t-n}^{t-1}.
\end{align}
**** Non-linear models
We could have e.g. a neural network with the n-step window as the input.

*** Variable order Markov models
Having a long window size / order increases the number of parameters.
Variable order models allow us to:
- Look far into the past.
- Ignore the distant past when necessary.

**** Variable order methods
- Probabilstic: tree-structured models.
- Neural networks: attention mechanisms.

**** Common idea: context
- $c  = f(x_1, \ldots, x_{t-1})$: a sparse vector pointing to important bits in the past
- $\hat{x}_t = g(x, c)$ prediction function only looking at $c$.

  
** Hidden Markov models
*** Hidden Markov Model
**** Generic Hidden Markov Model
\begin{tikzpicture}
\node[RV] at (-1,1) (x0) {$x_{t-1}$};
\node[RV] at (0,1) (x1) {$x_t$};
\node[RV] at (1,1) (x2) {$x_{t+1}$};
\node[RV,hidden] at (-1,0) (s0) {$s_{t-1}$};
\node[RV,hidden] at (0,0) (s1) {$s_t$};
\node[RV,hidden] at (1,0) (s2) {$s_{t+1}$};
\draw[->] (s0) to (x0);
\draw[->] (s1) to (x1);
\draw[->] (s2) to (x2);
\draw[->] (s0) to (s1);
\draw[->] (s1) to (s2);
\end{tikzpicture}
- $s_t$: The state of the model. It has the \alert{Markov} property.
- $x_t$: The observtions of the model. They are \alert{not} Markov.
- When the model parameters are known, we can infer the hidden states. This is called \alert{filtering}.
**** Different types of HMMs
- Discrete $s_t, x_t$: Used in string prediction.
- Continuous $s_t, x_t$: The Kalman filter. Used in automatic control. Also RNNs.
- Discrete $s_t$, continuous $x_t$. Used in speech recognition.
  

** Recommendation systems
*** Recommendation systems
**** The setting
- At time t, client $x_t$ appears
- We give a recommendation $a_t$.
- The customer chooses $y_t$
- We obtain a reward $r_t = \rho(a_t, y_t) \in \Reals$.
**** The two problems in recommendation
- How to model user preferences
- What to recommend

*** The modelling problem
**** The setting
- Clients $x_t$ 
- Items $y_t$
- Ratings $z_t$

* Hierarchical models

** Hierarchical models
*** Multiple hypotheses
[not covered]


