{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Computer vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "h , w = 28, 28\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot image\n",
    "\n",
    "Get a random image from the dataset and plot it using matplotlib.\n",
    "Remember that after selecting an image you have to transform it into a matrix if (28,28) pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = X.iloc[15].values # this is a vector we want matrix\n",
    "y_example = y.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image, cmap = \"Greys\")\n",
    "plt.title(f\"Example Digit, label = {y_example}\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Height\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise data \n",
    "\n",
    "Normalize your data in order to scale the values from [0,1]\n",
    "In order to do so think of the maximum and the minimum value of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm =  # convert image to value [0, 1] \n",
    "y = y.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train val test\n",
    "\n",
    "Split the dataset into training validation and test set using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# create train test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  (50400, 784)\n",
      "val set size:  (5600, 784)\n",
      "test set size:  (14000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"train set size: \", X_train.shape)\n",
    "print(\"val set size: \", X_val.shape)\n",
    "print(\"test set size: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert image to pytorch dataset\n",
    "\n",
    "Fill the following function that get a pytorch data loader from a numpy arrays.\n",
    "You can check our previous pytorch lab to write the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_dataloader(X,y, batch_size, shuffle ):\n",
    "    # fill your code\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the loader for the different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader(X_train.values, y_train.values, batch_size = 16, shuffle=True)\n",
    "val_dataloader = get_dataloader(X_val.values, y_val.values, batch_size = y_val.shape[0], shuffle=False)\n",
    "test_dataloader = get_dataloader(X_test.values, y_test.values, batch_size = y_test.shape[0], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "Implement the softmax function.\n",
    "The function in applied to a array $z$ and return a normalised array with values between $[0,1]$\n",
    "\n",
    "The values of each element is calculated according to the following formula:\n",
    "\n",
    "$$ \\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K $$\n",
    "\n",
    "The advantage of softmax is that is we sum the result of the function always sum to one.\n",
    "So is used in machine learning problem in order to convert a vector of arbitrary values into a vector of propabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # fill your code\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1 = np.array([1,1,1])\n",
    "softmax(array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21194156, 0.21194156, 0.57611688])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = np.array([0,0,1])\n",
    "softmax(array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.92508923e-04, 2.42609079e-03, 1.79265209e-02, 9.78754879e-01])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = np.array([1, 2, 4, 8])\n",
    "softmax(array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model\n",
    "\n",
    "Define a simple neural network, as our the example in introduction to pytorch.\n",
    "Use 100 units for the first layer and 50 for the second layer.  \n",
    "The output units must be the same as the number of classes (10).  \n",
    "We also have to use softmax to transform the output in range $[0, 1]$ in order to represent propabilities as explained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNeuralNetwork\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Simple two layer neural network for regression\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_input_features):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple two layer neural network for regression\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1\n",
    "\n",
    "        \n",
    "        # layer  2\n",
    "\n",
    "        \n",
    "        # layer output layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NeuralNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m simple_ff_model \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNetwork\u001b[49m(num_input_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NeuralNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "simple_ff_model = NeuralNetwork(num_input_features = X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Training  Step----------------------------\n",
    "def training_step(model, input_data, optimizer, loss_fn):\n",
    "    # reset gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # unfold data\n",
    "    x_batch, y_batch = input_data\n",
    "    \n",
    "    # get predictions\n",
    "    y_pred_propa = model(x_batch)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = loss_fn(y_pred_propa, y_batch)\n",
    "\n",
    "    # compute gradients \n",
    "    loss.backward()\n",
    "    \n",
    "    # optimise network\n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute metrics for monitoring\n",
    "    with torch.no_grad(): \n",
    "        y_pred = torch.argmax(y_pred_propa,axis=1)\n",
    "        \n",
    "        train_acc = torch.sum(y_pred == y_batch) / y_batch.shape[0]\n",
    "\n",
    "    return loss.data.numpy(), train_acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_step(model, input_data, loss_fn):\n",
    "    # when we use torch.no_grad pytorch didnt store information\n",
    "    # that is required to calculate gradients so is fasterr \n",
    "    with torch.no_grad(): \n",
    "        x_batch, y_batch = input_data\n",
    "        y_pred_proba = model(x_batch)\n",
    "        loss = loss_fn(y_pred_proba, y_batch)\n",
    "\n",
    "        # compute metrics\n",
    "        y_pred = torch.argmax(y_pred_proba,axis=1)\n",
    "        acc = torch.sum(y_pred == y_batch) / y_batch.shape[0]\n",
    "    return loss.data.numpy(), acc.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# -------------------- Train  Loop----------------------------\n",
    "def train_loop(train_dataloader, val_dataloader, patient, epochs, model, optimizer, loss_fn):\n",
    "    best_loss = np.inf\n",
    "    consecutive_epoch = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(epochs): # iterate over epoch    \n",
    "\n",
    "        # -------------------- Training on each epoch ----------------------------\n",
    "        total_step = len(train_dataloader)\n",
    "        accumulated_loss = 0 # monitor loss during training\n",
    "        accumulated_accuracy = 0 # monitor  accuracy during training\n",
    "        accuracy_list = []\n",
    "        start = time.time()\n",
    "        for step in range(total_step): # iterate over batch\n",
    "            batch_data = next(iter(train_dataloader)) # get a batch\n",
    "            loss, accuracy = training_step(model,batch_data,optimizer,loss_fn) # train model using a single batch\n",
    "            accuracy_list += [accuracy]\n",
    "            accumulated_loss = (step * accumulated_loss + loss)/(step+1)\n",
    "            accumulated_accuracy =  (step * accumulated_accuracy + accuracy)/(step+1)\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "        train_history += [{\"loss\":accumulated_loss, \"accuracy\":accumulated_accuracy, \"epoch\": epoch, \"set\":\"train\"}]\n",
    "\n",
    "        # -------------------- Monitor Error Validation set ----------------------------\n",
    "        val_data = next(iter(val_dataloader))\n",
    "        val_loss, val_accuracy = evaluation_step(model, val_data, loss_fn)\n",
    "        val_history += [{\"loss\":val_loss, \"accuracy\":val_accuracy, \"epoch\": epoch, \"set\":\"val\"}]\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}:({total_time:.3f} sec)  loss:{accumulated_loss:.3f}, accuracy-:{accumulated_accuracy:.3f}, val_loss:{val_loss:.3f}, val_accuracy->{val_accuracy:.3f}\")\n",
    "\n",
    "        # -------------------- Early Stoping ----------------------------\n",
    "        if val_loss > best_loss:\n",
    "            consecutive_epoch += 1\n",
    "        else:\n",
    "            best_loss = val_loss # we have an improvement\n",
    "            consecutive_epoch = 0 # reset counter\n",
    "            best_epoch = epoch\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "        if consecutive_epoch > patient:\n",
    "            break\n",
    "    val_history_df = pd.DataFrame(val_history)\n",
    "    train_history_df = pd.DataFrame(train_history)\n",
    "    return model, val_history_df, train_history_df, best_loss, best_epoch, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10:(5.312 sec)  loss:1.673, accuracy-:0.796, val_loss:1.621, val_accuracy->0.841\n",
      "Epoch 1/10:(5.187 sec)  loss:1.559, accuracy-:0.904, val_loss:1.529, val_accuracy->0.933\n",
      "Epoch 2/10:(5.171 sec)  loss:1.506, accuracy-:0.957, val_loss:1.518, val_accuracy->0.946\n",
      "Epoch 3/10:(5.188 sec)  loss:1.497, accuracy-:0.965, val_loss:1.509, val_accuracy->0.952\n",
      "Epoch 4/10:(5.107 sec)  loss:1.493, accuracy-:0.969, val_loss:1.504, val_accuracy->0.959\n",
      "Epoch 5/10:(5.202 sec)  loss:1.488, accuracy-:0.974, val_loss:1.499, val_accuracy->0.962\n",
      "Epoch 6/10:(5.265 sec)  loss:1.486, accuracy-:0.975, val_loss:1.501, val_accuracy->0.961\n",
      "Epoch 7/10:(5.266 sec)  loss:1.485, accuracy-:0.976, val_loss:1.500, val_accuracy->0.961\n",
      "Epoch 8/10:(5.207 sec)  loss:1.484, accuracy-:0.977, val_loss:1.498, val_accuracy->0.964\n",
      "Epoch 9/10:(5.097 sec)  loss:1.482, accuracy-:0.979, val_loss:1.502, val_accuracy->0.960\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(simple_ff_model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # cross entropy\n",
    "\n",
    "out = train_loop(train_dataloader = train_dataloader,\n",
    "                 val_dataloader = val_dataloader,\n",
    "                 patient = 3, \n",
    "                 epochs = 10,\n",
    "                 model = simple_ff_model,\n",
    "                 optimizer= optimizer,\n",
    "                 loss_fn= loss_fn)\n",
    "\n",
    "model, val_history_es_5, train_history_es_5, best_loss, best_epoch, best_weights = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreasathanasopoulos/Phd/projects/bayesian_fairness/envs/bayesian-fairness/lib/python3.9/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_data = next(iter(test_dataloader))\n",
    "loss, acc = evaluation_step(simple_ff_model, test_data, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1.5008513, dtype=float32), array(0.96021426, dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cnn model\n",
    "\n",
    "Define a cnn architecture, with two cnn layers using 16 and 32 kernels respectively.\n",
    "Each kernel on both layers in has a size 5x5.\n",
    "After each layer we apply Relu activation function and max-pooling with kernel size 2 and stride 2.\n",
    "\n",
    "At the end you add a simple linear layer on the flatten representation of the final cnn architecture.\n",
    "\n",
    "Check the supplementary slide to calculate the correct input shape for you layers, otherwise you will get an error. (final slides)\n",
    "\n",
    "Fill the code bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNeuralNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple two layer neural network for regression\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        self.cnn_layer_1 = torch.nn.Conv2d(in_channels=,    # input channels           \n",
    "                                           out_channels=,   # number of kernels\n",
    "                                           kernel_size=  # kenrel size \n",
    "                                          )\n",
    "        self.activation_1 = # relu\n",
    "        self.polling_1 = torch.nn.MaxPool2d(kernel_size=) # maxpolling\n",
    "        \n",
    "        # layer  2\n",
    "        self.cnn_layer_2 = torch.nn.Conv2d(\n",
    "                                          in_channels=,  # input channels           \n",
    "                                          out_channels=, # number of kernels        \n",
    "                                          kernel_size= # kenrel size \n",
    "                                          )\n",
    "        self.activation_2 =  # relu\n",
    "        self.polling_2 = torch.nn.MaxPool2d() # maxpolling\n",
    "        \n",
    "        # layer output layer\n",
    "        self.out_layer = torch.nn.Linear(in_features=, # inputs that depends on the output of the cnn\n",
    "                                         out_features=)\n",
    "        self.out_activation= torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # construct the forward pass\n",
    "        x_1 = self.polling_1(self.activation_1( self.cnn_layer_1( x ) ))\n",
    "        x_2 = self.polling_2(self.activation_2( self.cnn_layer_2( x_1 ) ))\n",
    "        \n",
    "        flaten = torch.flatten(x_2,start_dim=1)\n",
    "        \n",
    "        output = self.out_activation(self.out_layer(flaten))\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CnnNeuralNetwork(num_of_channels = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcnn_model\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data loader for the cnn model\n",
    "\n",
    "The cnn model takes as input a batch of images of (28,28) shape.\n",
    "In pytorch use the first dimension of the input as the channel information, so it receives input batches of shape: (batch_size, Channels, H, W)\n",
    "Call the get_dataloader with the right arguments in order to produce the input in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader(# add corect argument,\n",
    "                                  y_train.values,\n",
    "                                  batch_size = 16,\n",
    "                                  shuffle=True)\n",
    "\n",
    "val_dataloader = get_dataloader(# add corect argument,\n",
    "                                y_val.values,\n",
    "                                batch_size = y_val.shape[0],\n",
    "                                shuffle=False)\n",
    "\n",
    "test_dataloader = get_dataloader(# add corect argument,\n",
    "                                 y_test.values,\n",
    "                                 batch_size = y_test.shape[0],\n",
    "                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/15:(13.262 sec)  loss:1.592, accuracy-:0.872, val_loss:1.492, val_accuracy->0.970\n",
      "Epoch 1/15:(13.120 sec)  loss:1.485, accuracy-:0.977, val_loss:1.488, val_accuracy->0.975\n",
      "Epoch 2/15:(13.136 sec)  loss:1.480, accuracy-:0.982, val_loss:1.492, val_accuracy->0.971\n",
      "Epoch 3/15:(12.994 sec)  loss:1.477, accuracy-:0.985, val_loss:1.481, val_accuracy->0.980\n",
      "Epoch 4/15:(12.980 sec)  loss:1.475, accuracy-:0.987, val_loss:1.479, val_accuracy->0.982\n",
      "Epoch 5/15:(13.429 sec)  loss:1.474, accuracy-:0.987, val_loss:1.478, val_accuracy->0.983\n",
      "Epoch 6/15:(13.001 sec)  loss:1.472, accuracy-:0.989, val_loss:1.480, val_accuracy->0.981\n",
      "Epoch 7/15:(13.318 sec)  loss:1.471, accuracy-:0.990, val_loss:1.478, val_accuracy->0.983\n",
      "Epoch 8/15:(13.117 sec)  loss:1.470, accuracy-:0.991, val_loss:1.474, val_accuracy->0.988\n",
      "Epoch 9/15:(12.985 sec)  loss:1.470, accuracy-:0.991, val_loss:1.479, val_accuracy->0.982\n",
      "Epoch 10/15:(13.033 sec)  loss:1.470, accuracy-:0.991, val_loss:1.475, val_accuracy->0.986\n",
      "Epoch 11/15:(12.998 sec)  loss:1.470, accuracy-:0.991, val_loss:1.472, val_accuracy->0.989\n",
      "Epoch 12/15:(12.981 sec)  loss:1.470, accuracy-:0.992, val_loss:1.475, val_accuracy->0.987\n",
      "Epoch 13/15:(13.086 sec)  loss:1.469, accuracy-:0.992, val_loss:1.474, val_accuracy->0.987\n",
      "Epoch 14/15:(13.007 sec)  loss:1.469, accuracy-:0.993, val_loss:1.475, val_accuracy->0.986\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # binary cross entropy\n",
    "\n",
    "out = train_loop(train_dataloader = train_dataloader,\n",
    "                 val_dataloader = val_dataloader,\n",
    "                 patient = 3, \n",
    "                 epochs = 15,\n",
    "                 model = cnn_model,\n",
    "                 optimizer= optimizer,\n",
    "                 loss_fn= loss_fn)\n",
    "\n",
    "model, val_history_es_5, train_history_es_5, best_loss, best_epoch, best_weights = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = next(iter(test_dataloader))\n",
    "loss, acc = evaluation_step(cnn_model, test_data, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
