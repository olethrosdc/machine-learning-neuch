{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ce0ae9",
   "metadata": {},
   "source": [
    "# Lab 3 - Logistic regression Assignment\n",
    "\n",
    "In this assignment, we will implement a logistic regression classifier for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7bb90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:43:21.782666Z",
     "start_time": "2024-10-02T12:43:21.684892Z"
    },
    "collapsed": true
   },
   "source": [
    "### 1. Data - Breast Cancer dataset\n",
    "\n",
    "The Breast Cancer dataset contains features computed from digitized images of breast masses. \n",
    "It has 30 numerical features describing characteristics such as radius, texture, and smoothness. \n",
    "\n",
    "The target variable is binary, indicating whether a tumor is malignant (1) or benign (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691d4469",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-17T06:03:15.080076Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Add target column\n",
    "df['target'] = y\n",
    "\n",
    "# Show first rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdceb4cce587cb",
   "metadata": {},
   "source": [
    "we can now normalise our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751d1af6e81e447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T14:32:36.058514Z",
     "start_time": "2025-10-09T14:32:36.033777Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df[feature_names])\n",
    "X_scaled = scaler.transform(df[feature_names])\n",
    "\n",
    "# Convert back to DataFrame (optional)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316a555e89fe0889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T14:32:44.788544Z",
     "start_time": "2025-10-09T14:32:44.776469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.097064</td>\n",
       "      <td>-2.073335</td>\n",
       "      <td>1.269934</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "      <td>3.283515</td>\n",
       "      <td>2.652874</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>2.217515</td>\n",
       "      <td>2.255747</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886690</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>2.001237</td>\n",
       "      <td>1.307686</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>2.109526</td>\n",
       "      <td>2.296076</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.937015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.829821</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>1.685955</td>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "      <td>-0.487072</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>-0.868652</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805927</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>1.535126</td>\n",
       "      <td>1.890489</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>1.087084</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>0.281190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.579888</td>\n",
       "      <td>0.456187</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "      <td>1.052926</td>\n",
       "      <td>1.363478</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>0.939685</td>\n",
       "      <td>-0.398008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511870</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.347475</td>\n",
       "      <td>1.456285</td>\n",
       "      <td>0.527407</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>0.854974</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.201391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.768909</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>-0.592687</td>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "      <td>3.402909</td>\n",
       "      <td>1.915897</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>2.867383</td>\n",
       "      <td>4.910919</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>-0.550021</td>\n",
       "      <td>3.394275</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.989588</td>\n",
       "      <td>2.175786</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>4.935010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.750297</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>1.776573</td>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>0.539340</td>\n",
       "      <td>1.371011</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>-0.562450</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>-1.466770</td>\n",
       "      <td>1.338539</td>\n",
       "      <td>1.220724</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.313395</td>\n",
       "      <td>0.613179</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>-0.868353</td>\n",
       "      <td>-0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>2.110995</td>\n",
       "      <td>0.721473</td>\n",
       "      <td>2.060786</td>\n",
       "      <td>2.343856</td>\n",
       "      <td>1.041842</td>\n",
       "      <td>0.219060</td>\n",
       "      <td>1.947285</td>\n",
       "      <td>2.320965</td>\n",
       "      <td>-0.312589</td>\n",
       "      <td>-0.931027</td>\n",
       "      <td>...</td>\n",
       "      <td>1.901185</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>1.752563</td>\n",
       "      <td>2.015301</td>\n",
       "      <td>0.378365</td>\n",
       "      <td>-0.273318</td>\n",
       "      <td>0.664512</td>\n",
       "      <td>1.629151</td>\n",
       "      <td>-1.360158</td>\n",
       "      <td>-0.709091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1.704854</td>\n",
       "      <td>2.085134</td>\n",
       "      <td>1.615931</td>\n",
       "      <td>1.723842</td>\n",
       "      <td>0.102458</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>1.263669</td>\n",
       "      <td>-0.217664</td>\n",
       "      <td>-1.058611</td>\n",
       "      <td>...</td>\n",
       "      <td>1.536720</td>\n",
       "      <td>2.047399</td>\n",
       "      <td>1.421940</td>\n",
       "      <td>1.494959</td>\n",
       "      <td>-0.691230</td>\n",
       "      <td>-0.394820</td>\n",
       "      <td>0.236573</td>\n",
       "      <td>0.733827</td>\n",
       "      <td>-0.531855</td>\n",
       "      <td>-0.973978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.702284</td>\n",
       "      <td>2.045574</td>\n",
       "      <td>0.672676</td>\n",
       "      <td>0.577953</td>\n",
       "      <td>-0.840484</td>\n",
       "      <td>-0.038680</td>\n",
       "      <td>0.046588</td>\n",
       "      <td>0.105777</td>\n",
       "      <td>-0.809117</td>\n",
       "      <td>-0.895587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561361</td>\n",
       "      <td>1.374854</td>\n",
       "      <td>0.579001</td>\n",
       "      <td>0.427906</td>\n",
       "      <td>-0.809587</td>\n",
       "      <td>0.350735</td>\n",
       "      <td>0.326767</td>\n",
       "      <td>0.414069</td>\n",
       "      <td>-1.104549</td>\n",
       "      <td>-0.318409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1.838341</td>\n",
       "      <td>2.336457</td>\n",
       "      <td>1.982524</td>\n",
       "      <td>1.735218</td>\n",
       "      <td>1.525767</td>\n",
       "      <td>3.272144</td>\n",
       "      <td>3.296944</td>\n",
       "      <td>2.658866</td>\n",
       "      <td>2.137194</td>\n",
       "      <td>1.043695</td>\n",
       "      <td>...</td>\n",
       "      <td>1.961239</td>\n",
       "      <td>2.237926</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>1.653171</td>\n",
       "      <td>1.430427</td>\n",
       "      <td>3.904848</td>\n",
       "      <td>3.197605</td>\n",
       "      <td>2.289985</td>\n",
       "      <td>1.919083</td>\n",
       "      <td>2.219635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-1.808401</td>\n",
       "      <td>1.221792</td>\n",
       "      <td>-1.814389</td>\n",
       "      <td>-1.347789</td>\n",
       "      <td>-3.112085</td>\n",
       "      <td>-1.150752</td>\n",
       "      <td>-1.114873</td>\n",
       "      <td>-1.261820</td>\n",
       "      <td>-0.820070</td>\n",
       "      <td>-0.561032</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.410893</td>\n",
       "      <td>0.764190</td>\n",
       "      <td>-1.432735</td>\n",
       "      <td>-1.075813</td>\n",
       "      <td>-1.859019</td>\n",
       "      <td>-1.207552</td>\n",
       "      <td>-1.305831</td>\n",
       "      <td>-1.745063</td>\n",
       "      <td>-0.048138</td>\n",
       "      <td>-0.751207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0       1.097064     -2.073335        1.269934   0.984375         1.568466   \n",
       "1       1.829821     -0.353632        1.685955   1.908708        -0.826962   \n",
       "2       1.579888      0.456187        1.566503   1.558884         0.942210   \n",
       "3      -0.768909      0.253732       -0.592687  -0.764464         3.283553   \n",
       "4       1.750297     -1.151816        1.776573   1.826229         0.280372   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564     2.110995      0.721473        2.060786   2.343856         1.041842   \n",
       "565     1.704854      2.085134        1.615931   1.723842         0.102458   \n",
       "566     0.702284      2.045574        0.672676   0.577953        -0.840484   \n",
       "567     1.838341      2.336457        1.982524   1.735218         1.525767   \n",
       "568    -1.808401      1.221792       -1.814389  -1.347789        -3.112085   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            3.283515        2.652874             2.532475       2.217515   \n",
       "1           -0.487072       -0.023846             0.548144       0.001392   \n",
       "2            1.052926        1.363478             2.037231       0.939685   \n",
       "3            3.402909        1.915897             1.451707       2.867383   \n",
       "4            0.539340        1.371011             1.428493      -0.009560   \n",
       "..                ...             ...                  ...            ...   \n",
       "564          0.219060        1.947285             2.320965      -0.312589   \n",
       "565         -0.017833        0.693043             1.263669      -0.217664   \n",
       "566         -0.038680        0.046588             0.105777      -0.809117   \n",
       "567          3.272144        3.296944             2.658866       2.137194   \n",
       "568         -1.150752       -1.114873            -1.261820      -0.820070   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                  2.255747  ...      1.886690      -1.359293   \n",
       "1                 -0.868652  ...      1.805927      -0.369203   \n",
       "2                 -0.398008  ...      1.511870      -0.023974   \n",
       "3                  4.910919  ...     -0.281464       0.133984   \n",
       "4                 -0.562450  ...      1.298575      -1.466770   \n",
       "..                      ...  ...           ...            ...   \n",
       "564               -0.931027  ...      1.901185       0.117700   \n",
       "565               -1.058611  ...      1.536720       2.047399   \n",
       "566               -0.895587  ...      0.561361       1.374854   \n",
       "567                1.043695  ...      1.961239       2.237926   \n",
       "568               -0.561032  ...     -1.410893       0.764190   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0           2.303601    2.001237          1.307686           2.616665   \n",
       "1           1.535126    1.890489         -0.375612          -0.430444   \n",
       "2           1.347475    1.456285          0.527407           1.082932   \n",
       "3          -0.249939   -0.550021          3.394275           3.893397   \n",
       "4           1.338539    1.220724          0.220556          -0.313395   \n",
       "..               ...         ...               ...                ...   \n",
       "564         1.752563    2.015301          0.378365          -0.273318   \n",
       "565         1.421940    1.494959         -0.691230          -0.394820   \n",
       "566         0.579001    0.427906         -0.809587           0.350735   \n",
       "567         2.303601    1.653171          1.430427           3.904848   \n",
       "568        -1.432735   -1.075813         -1.859019          -1.207552   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0           2.109526              2.296076        2.750622   \n",
       "1          -0.146749              1.087084       -0.243890   \n",
       "2           0.854974              1.955000        1.152255   \n",
       "3           1.989588              2.175786        6.046041   \n",
       "4           0.613179              0.729259       -0.868353   \n",
       "..               ...                   ...             ...   \n",
       "564         0.664512              1.629151       -1.360158   \n",
       "565         0.236573              0.733827       -0.531855   \n",
       "566         0.326767              0.414069       -1.104549   \n",
       "567         3.197605              2.289985        1.919083   \n",
       "568        -1.305831             -1.745063       -0.048138   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                   1.937015  \n",
       "1                   0.281190  \n",
       "2                   0.201391  \n",
       "3                   4.935010  \n",
       "4                  -0.397100  \n",
       "..                       ...  \n",
       "564                -0.709091  \n",
       "565                -0.973978  \n",
       "566                -0.318409  \n",
       "567                 2.219635  \n",
       "568                -0.751207  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf33e78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "..      ...\n",
       "564       0\n",
       "565       0\n",
       "566       0\n",
       "567       0\n",
       "568       1\n",
       "\n",
       "[569 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e26969ff0832bbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T14:44:18.376464Z",
     "start_time": "2025-10-09T14:44:18.359082Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[feature_names], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60872a4fb301308e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T14:44:18.930718Z",
     "start_time": "2025-10-09T14:44:18.896864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize and train logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f9374",
   "metadata": {},
   "source": [
    "### Assigment - Implement Logistic regression\n",
    "\n",
    "In the assigment you will ask to impment your own logistic regression model.\n",
    "This is similar to what we demostrate for the percepton algorithm.\n",
    "\n",
    "you will ask to fill the following class in steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d340dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T15:51:27.648732Z",
     "start_time": "2025-10-09T15:51:27.636902Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, n_features):\n",
    "        self.lr = None            # learning rate\n",
    "        self.n_iter = None    # number of iterations\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, n_iter=1000):\n",
    "        # data shape\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.lr = lr            # learning rate\n",
    "        self.n_iter = n_iter    # number of iterations\n",
    "        \n",
    "        # initialise model parameters\n",
    "        self.weights = np.zeros(self.n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iter):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(linear_model)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.where(probs >= threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810779cbb7977ec",
   "metadata": {},
   "source": [
    "### **Step 1 — Implement `predict_proba`**\n",
    "\n",
    "In this step, you will implement the `predict_proba` method of the `LogisticRegression` class.  \n",
    "This method should return the **predicted probabilities** that each sample belongs to class `1`.\n",
    "\n",
    "The logistic regression model predicts probabilities using the **sigmoid function**:\n",
    "\n",
    "$\\hat{y} = Pr(y=1|x) = \\sigma(Xw + b) , \\quad \\text{where } \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Here:  \n",
    "- $X$ is the input data matrix (shape: *n_samples × n_features*).  \n",
    "- $w$ is the weight vector (learned parameters).  \n",
    "- $b$ is the bias term.  \n",
    "- $\\sigma$ is the sigmoid function\n",
    "\n",
    "#### **Implementation Steps**\n",
    "1. Compute the **linear combination** $z = Xw + b$.  \n",
    "2. Apply the **sigmoid function** to obtain probabilities.  \n",
    "3. Return these probabilities as a NumPy array.  \n",
    "4. Make sure your code works **for a batch of data**, not just a single sample.  \n",
    "   - $X$ should be a 2D array.  \n",
    "   - Use matrix–vector operations (e.g., `np.dot(X, w) + b`) to ensure correct broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c3177bc20c15f35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T15:51:28.920817Z",
     "start_time": "2025-10-09T15:51:28.916368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming X_train, X_test, y_train, y_test are ready and scaled\n",
    "model = MyLogisticRegression(n_features= X_train.shape[1])\n",
    "y_probs = model.predict_proba(X_test.iloc[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677c535bcbbcddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T06:31:51.915586Z",
     "start_time": "2025-10-10T06:31:51.874041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the outputs\n",
    "y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547dbf734279e818",
   "metadata": {},
   "source": [
    "### **Step 2 — Implement `predict`**\n",
    "\n",
    "In this step, you will implement the `predict` method of the `LogisticRegression` class.  \n",
    "This method should return the **predicted class labels** (`0` or `1`) for each input sample.\n",
    "\n",
    "The `predict` method relies on the probabilities obtained from your `predict_proba` method.\n",
    "\n",
    "#### **Implementation Steps**\n",
    "1. Use your `predict_proba` method to compute the predicted probabilities $\\hat{y}$ for all samples.  \n",
    "2. Apply a **decision threshold** (commonly 0.5):  \n",
    "   - If $\\hat{y} \\geq 0.5$, predict class `1`.  \n",
    "   - Otherwise, predict class `0`.  \n",
    "3. Return the predicted labels as a NumPy array of integers (`dtype=int`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f5dde00c206fd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T15:51:29.843377Z",
     "start_time": "2025-10-09T15:51:29.839412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test.iloc[0:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264136e58985eb4",
   "metadata": {},
   "source": [
    "### **Step 3 — Implement `fit`**\n",
    "\n",
    "In this step, you will implement the `fit` method of the `LogisticRegression` class.  \n",
    "This method should train the model using **gradient descent** to find the optimal parameters $w$ and $b$ that minimize the **binary cross-entropy loss**.\n",
    "\n",
    "You want the version of  **gradient descent** where the every update is based to the whole dataset.\n",
    "\n",
    "The loss function for logistic regression is:\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{n} \\sum_{i=1}^{n} \\big[ y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i) \\big]\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_i = \\sigma(X_i w + b)$.\n",
    "\n",
    "#### **Implementation Steps**\n",
    "1. **Initialize the parameters**:\n",
    "   - Set $w$ as a zero vector of shape `(n_features,)`.\n",
    "   - Set $b = 0$.\n",
    "2. **For a fixed number of iterations (epochs)**:\n",
    "   - Use the whole dataset:\n",
    "     - Compute the predicted probabilities: $\\hat{y} = \\sigma(X w + b)$ using your `predict_proba` function.\n",
    "     - Compute the **gradients** of the loss with respect to $w$ and $b$:  \n",
    "       - $\\displaystyle \\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) X_i$, this is a vector of size (n_features,1)\n",
    "       - $\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)$\n",
    "     - Update the parameters using the learning rate $\\alpha$:  \n",
    "       - $w = w - \\alpha \\frac{\\partial L}{\\partial w}$  \n",
    "       - $b = b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "> ⚠️ Make sure your implementation works for a batch of samples (matrix $X$). Avoid using explicit `for` loops.\n",
    "> ⚠️ You **do not need to compute the loss $L$** since the gradients are known in closed form.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9427d73418677b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T15:52:07.882986Z",
     "start_time": "2025-10-09T15:52:06.096350Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X=X_train, y=y_train, lr=0.01, n_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac49f4a65073e7",
   "metadata": {},
   "source": [
    "### **Step 4 — Evaluate the Model**\n",
    "\n",
    "Calculate the **test accuracy** of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5382c311f1b93071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T15:52:07.894305Z",
     "start_time": "2025-10-09T15:52:07.887694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fad94adccc3de3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
