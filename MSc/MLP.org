#+TITLE: Multi-Layer Perceptrons and Deep Learning
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \input{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[currentsection]\end{frame}}
#+name: setup-minted
* Features and layers
** Introduction
*** Perceptron vs linear regression
**** Network architecture                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
\begin{center}
\begin{tikzpicture}
      \node[RV] at (0,0) (x1) {$x_1$};
      \node[RV] at (1,0) (x2) {$x_2$};
      \node[RV] at (0,-1) (y1) {$\hy$};
      \draw[->] (x1) to (y1);
      \draw[->] (x2) to (y1);
\end{tikzpicture}
\end{center}
#+ATTR_BEAMER: :overlay <+->
- Network output
\[
\hy = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]
- Chain rule
\[
\nabla_\beta \loss = \nabla_{\hy} \loss \nabla_\beta \hy
\]
- Network gradient
\[
\nabla_\beta \hy = (x_1, x_2)
\]
**** Cost functions:                                                  :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
***** Cost functions
The only difference are the cost functions
- Perceptron
\[
\loss  = - \ind{y \neq \hy}\hy
\]
with
\[
\nabla \loss  = - \ind{y \neq \hy} y x
\]

- Linear regression
\[
\loss = (\hy - y)^2,
\]
with
\[
\nabla_{\hy} \loss = 2(\hy - y).
\]

** Layers
*** Layering and features
**** Layering and features                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
***** Fixed layers
 - Input to layer $x \in R^n$ 
 - Output from layer $\hby \in R^m$.

***** Intermediate layers
 - Linear layer
 - Non-linear *activation* function.

***** Linear layers types
 - Dense 
 - Sparse
 - Convolutional

***** Activation funnction
 - Sigmoid
 - Softmax
**** The graph                                                        :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
\begin{tikzpicture}
      \node[RV] at (0,0) (x1) {$x_1$};
      \node[RV] at (1,0) (x2) {$x_2$};
      \node[draw] at (4,0) {Input layer};
      \node[RV] at (0,-1) (w1) {$w_1$};
      \node[RV] at (1,-1) (w2) {$w_2$};
      \node[draw] at (4,-1) {Linear layer};
      \node[RV] at (0,-2) (z1) {$z_1$};
      \node[RV] at (1,-2) (z2) {$z_2$};
      \node[draw] at (4,-2) {Sigmoid activation};
      \node[RV] at (0,-3) (v1) {$v_1$};
      \node[RV] at (1,-3) (v2) {$v_2$};
      \node[draw] at (4,-3) {Linear layer};
      \node[RV] at (0,-4) (y1) {$\hy_1$};
      \node[RV] at (1,-4) (y2) {$\hy_2$};
      \node[draw] at (4,-4) {Softmax activation};
      \draw[->] (x1) to (w1);
      \draw[->] (x2) to (w1);
      \draw[->] (x1) to (w2);
      \draw[->] (x2) to (w2);
      \draw[->] (w1) to (z1);
      \draw[->] (w2) to (z2);
      \draw[->] (z1) to (v1);
      \draw[->] (z1) to (v2);
      \draw[->] (z2) to (v1);
      \draw[->] (z2) to (v2);
      \draw[->] (v1) to (y1);
      \draw[->] (v1) to (y2);
      \draw[->] (v2) to (y1);
      \draw[->] (v2) to (y2);
\end{tikzpicture}
*** Linear layers
**** Example: Linear regression with $n$ inputs, $m$ outputs.
- Input: Features $\bx \in \Reals^n$
- Dense linear layer with $\mparam \in \Reals^{m \times n}$
- Output: $\hby \in \Reals^m$
**** Dense linear layer
- Parameters $\mparam = \begin{pmatrix}\vparam_1 \\ \vdots \\ \vparam_m \end{pmatrix}$,
- $\vparam_i = [\param_{i,1}, \ldots, \param_{i,n}]$, $\vparam_i$ connects the \(i\)-th output $y_i$ to the features $\bx$:
\[
y_i = \vparam_i \bx
\]
- In compact form:
\[
\by = \mparam \bx 
\]
*** Multiple linear layers
**** Repeated linear transformations are linear
It does not really help to have multiple linear layers one after the other. For example, if we transform  $x \in \Reals^n$ to $z \in \Reals^k$ to $y \in \Reals^m$ through two matrices
\begin{align}
z & = Ax, &&A \in \Reals^{k \times n}\\
y &= Bz, &&B \in \Reals^{m \times k}
\end{align}
We can rewrite $y$ as 
\begin{align}
y &= B(Ax) = (BA) x = Cx, && C \in \Reals^{m \times n}
\end{align}
where $C = BA$.

- Successive linear layers have no advantage normally.\footnote{Multi-task learning might be an exception.}
- However, we can interlace them with *non-linear activation functions*. 



** Activation functions
*** ReLU activation
**** Left column                                                      :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
- Activation function:
\[
f(x) = \max(0, x)
\]
- Derivative
\[
\frac{d}{dx} f(x) = \ind{x > 0}
\]
Typically used in the hidden layers of neural networks, as it is:
- Simple to calculate.
- Nonlinear.
- Its gradient never vanishes.

**** Right column                                                     :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

\begin{tikzpicture}[domain=-2:2]
  \draw[thin,->] (-2.2,0) -- (2.2,0);
  \draw[thin,->] (0,-0.2) -- (0,2.2);
  \draw[dotted] (-2.1,-0.1) grid (2.1,2.1);
  \draw[dashed,thick,color=blue]   plot (\x, {max(0,\x)})    node[right] {$f$};
  \draw[dashdotted,thick,color=red,samples at={-2,0,0.001,2}]  plot (\x, {\x > 0}) node[right] {$\frac{df}{dz}$};
\end{tikzpicture}

*** Sigmoid activation
**** Whay                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
***** Example: Logistic regression
- Input $\bx \in \Reals^n$
- Intermediate output: $z \in \Reals$,
\[
z = \sum_{i=1}^n \param_i x_i.
\]
- Output: sigmoid activation  $\hy \in [0,1]$.
\[
f(z) =  1/[1 + \exp(-z)].
\]
Now we can interpret $\hy = P_\vparam(y = 1 | x)$.
***** Loss function: negative log likelihood
\[
\cost(\hy, y) = - [\ind{y=1} \ln (\hy) + \ind{y=-1} \ln (1 - \hy)]
\]
**** Figure                                                           :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
\begin{tikzpicture}
      \node[RV] at (0,0) (x1) {$x_1$};
      \node[RV] at (1,0) (x2) {$x_2$};
      \node[draw] at (4,0) {Input layer};
      \node[RV] at (0.5,-1) (z) {$z$};
      \node[draw] at (4,-1) {Linear layer};
      \node[RV] at (0.5,-2) (y) {$\hy$};
      \node[draw] at (4,-2) {Sigmoid layer};
      \draw[->] (x1) to (z);
      \draw[->] (x2) to (z);
      \draw[->] (z) to (y);
\end{tikzpicture}

*** Softmax layer
**** Description                                                      :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
***** Example: Multivariate logistic regression with $m$ classes.
- Input: *Features* $\bx \in \Reals^n$
- Fully-connected *linear* activation layer 
\[
\bz = \mparam \bx, \qquad \mparam \in \Reals^{m \times n}.
\]
- *Softmax* output
\[
\hy_i = \frac{\exp(z_i)}{\sum_{j = 1 ^m} \exp(z_j)}
\]
We can also interpret this as
\[
\hy_i \defn  \Pr(y = i \mid \bx)
\]
with usual loss $\cost(\hy, y) = - \ln \hy_y$
**** Figure :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
\begin{tikzpicture}
      \node[RV] at (0,0) (x1) {$x_1$};
      \node[RV] at (1,0) (x2) {$x_2$};
      \node[draw] at (4,0) {Input layer};
      \node[RV] at (0,-1) (z1) {$z_1$};
      \node[RV] at (1,-1) (z2) {$z_2$};
      \node[draw] at (4,-1) {Linear layer};
      \node[RV] at (0,-2) (y1) {$\hy_1$};
      \node[RV] at (1,-2) (y2) {$\hy_2$};
      \node[draw] at (4,-2) {Softmax layer};
      \draw[->] (x1) to (z1);
      \draw[->] (x2) to (z1);
      \draw[->] (x1) to (z2);
      \draw[->] (x2) to (z2);
      \draw[->] (z1) to (y1);
      \draw[->] (z1) to (y2);
      \draw[->] (z2) to (y1);
      \draw[->] (z2) to (y2);
\end{tikzpicture}


* Algorithms
** Random projection
*** Random projections
- Features $x$
- Hidden layer activation $z$
- Output $y$
**** Hidden layer: Random projection
Here we project the input into a high-dimensional space
\[
z_i = \sgn(\vparam_i^\top x) = y_i
\]
where $\mparam = [\vparam_i]_{i=1}^m$, $\param_{i,j} \sim \Normal(0,1)$

**** The reason for random projections
- The high dimension makes it easier to learn.
- The randomness ensures we are not learning something spurious.

** Back propagation
*** Background on back-propagation
**** Gradient descent algorithm
- We need to minimise the expected value $\E_\vparam[\loss]$ of the loss function $\loss$
- Since we cannot calculate  $\E_\vparam[\loss]$, we use:
\[
\nabla_\vparam \E_\vparam[\loss]
\approx 
\frac{1}{T} \sum_{t=1}^T \nabla_\vparam \cost(x_t, y_t, \vparam).
\]
- We can then update our parameters to reduce the *empirical loss*
\[
\vparam_{t+1} = \vparam_t - \alpha_t \nabla_\vparam \cost(x_t, y_t, \vparam).
\]
**** The problem
- However $\cost$ is a complex function of $\vparam$.
- How can we obtain $\nabla_\vparam \cost$?
**** The solution
- Use the chain rule to "backpropagate" errors.

*** The chain rule of differentiation
#+ATTR_LATEX: :width 150px
[[../fig/liebniz.jpeg]]
[1673] Liebniz

*** Chain rule applied to the perceptron
#+ATTR_LATEX: :width 150px
[[../fig/rosenblatt.jpeg]]
[1976] Rosenblat
*** Chain rule for deep neural netowrks
#+ATTR_LATEX: :width 100px
[[../fig/werbos.jpg]]
[1982] Werbos
*** Backpropagation given a name
1986: Learning representations by back-propagating errors.
**** Rumel                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:
#+ATTR_LATEX: :width 100px
[[../fig/DERumelhart.png]]
Rumelhart
**** Hinton                                                           :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:
#+ATTR_LATEX: :width 75px
[[../fig/hinton.jpg]]
Hinton
**** Williams                                                         :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:
#+ATTR_LATEX: :width 100px
[[../fig/williams.jpg]]
Williams
*** Elementary back-propagation: linear regression
\begin{center}
\begin{tikzpicture}
      \node[RV] at (0,0) (x) {\alert<1>{$\bx$}};
      \node[RV] at (1,1) (w) {\alert<1>{$\vparam$}};
      \node[RV] at (2,0) (hy) {\alert<2>{$\hy$}};
      \node[utility] at (4,0) (c) {\alert<3>{$\cost$}};
      \node[RV] at (6,0) (y) {\alert<1,2>{$y$}};
      \draw[->] (x) to (hy);
      \draw[->] (w) to (hy);
      \draw[->] (hy) to (c);
      \draw[->] (y) to (c);
	  \draw [blue, ->] (c) to [bend right=45] node [above]  {\alert<4>{$\nabla_f(\cost)$}} (hy);
      \draw [blue, ->] (hy) to [bend right=45] node [above]  {\alert<5>{$\nabla_\vparam(\hy)$}} (w);
\end{tikzpicture}
\end{center}
- $f : X \to Y$, $\cost: Y \times Y \to \Reals$, chain rule: $\nabla_\vparam \cost = \nabla_\vparam f \nabla_{\hy} \cost$
- *Forward*: follow the arrows to calculate *variables*
\[
\alert<2>{\hy} \defn f(\alert<1>{\vparam, x}) = \sum_{i=1}^n \alert<1>{\param_i x_i}, \qquad\alert<3>{\cost}(\hy, y) = (\hy - y)^2
\]
#+BEAMER: \pause
#+BEAMER: \pause
#+BEAMER: \pause
- *Backward*: return to calculate the *gradients*
\begin{align}
\nabla_\vparam \ell(\hy, y) 
&=
\nabla_{\vparam} \alert<5>{f(\vparam, \bx)} \times \alert<4>{\nabla_{\hy} \cost(\hy, y)}
\\
&=
\nabla_\vparam f(\vparam, \bx) 
\times 2 [\hy - y] 
\end{align}
#+BEAMER: \pause
#+BEAMER: \pause
- Update:
\[
\vparam_{t+1} = \vparam_t - \alpha_t \times \nabla_\vparam \ell(\hy_t, y_t) 
\]






*** Gradient descent with /back-propagation/
- Dataset $D$, cost function $\loss = \sum_t \cost_t$
- Parameters $\mparam_1, \ldots, \mparam_k$ with $k$ layers
- Intermediate variables: $\bz_j = h_j(\bz_{j-1}, \mparam_j)$, $\bz_0 = \bx$, $\bz_k = \hby$.
#+BEAMER: \pause
**** Dependency  graph
\begin{center}
\begin{tikzpicture}
      \node[RV] at (0,0) (x) {$\bx$};
      \node[RV] at (1,0) (z1) {$\bz_1$};
      \node[RV] at (2,0) (z2) {$\bz_2$};
      \node[RV] at (1,1) (w1) {$\mparam_1$};
      \node[RV] at (2,1) (w2) {$\mparam_2$};
      \node[RV] at (3,0) (hy) {$\hby$};
      \node[RV] at (5,0) (y) {$\by$};
      \node[utility] at (4,0) (c) {$\cost$};
      \draw[->] (x) to (z1);
      \draw[->] (z1) to (z2);
      \draw[->] (w2) to (z2);
      \draw[->] (w1) to (z1);
      \draw[->] (z2) to (hy);
      \draw[->] (hy) to (c);
      \draw[->] (y) to (c);
\end{tikzpicture}
\end{center}
#+BEAMER: \pause
**** Backpropagation with steepest stochastic gradient descent
- Forward step: For $j = 1, \ldots, k$, calculate $\bz_j = h_j(k)$ and $\cost(\hby, \by)$
- Backward step: Calculate $\nabla_{\hby} \cost$ and $d_j \defn \nabla_{\mparam_j} \cost = \nabla_{\mparam_j} z_j d_{j+1}$ for $j = k \ldots, 1$
- Apply gradient: $\mparam_j  -\!= \alpha d_j$.
*** Other algorithms and gradients
**** Natural gradient
Defined for probabilistic models
**** ADAM
Exponential moving average of gradient and square gradients
**** BFGS: Broyden–Fletcher–Goldfarb–Shanno algorithm
Newton-like method

** Derivatives

*** Linear layer
**** Definition
This is a linear combination of inputs $x \in \Reals^n$ and parameter matrix $\mparam \in \Reals^{m \times n}$
where $\mparam = \begin{bmatrix}
	\vparam_1\\
        \vdots\\
	\vparam_i\\
	\vdots\\
	\vparam_m
\end{bmatrix}
=
\begin{bmatrix}
\param_{1,1} & \cdots & \param_{1,j} & \cdots & \param_{1,m}\\
\vdots  & \ddots & \vdots  & \ddots & \cdots \\
\param_{i,1} & \cdots & \param_{i,j} & \cdots & \param_{i,m}\\
\vdots  & \ddots & \ddots  & \ddots & \cdots \\ 	   
\param_{n,1} & \cdots & \param_{i,j} & \cdots & \param_{n,m}
\end{bmatrix}$

\[
f(\mparam, \bx) = \mparam \bx 
\qquad
f_i(\mparam, \bx)= \vparam_i \cdot \bx =  \sum_{j=1}^n \param_{i,j} x_j,
\]


**** Gradient 
Each partial derivative is simple:
\[
\frac{\partial}{\partial \param_{i,j}} f_k(\mparam, \bx)
=
\sum_{k=1}^n \frac{\partial}{\partial \param_{i,j}}  \param_{i,k} x_k
=
 x_j
\]


*** Sigmoid layer
- This layer is used for *binary classification*.
- It is used in the *logistic regression* model to obtain label probabilities.
\[
f(z) = 1 / (1 + \exp(-z))
\]
- Derivative
\[
\frac{d}{dz} f(z) = \exp(-z)/[1+\exp(-z)]^{2}
\]

\begin{tikzpicture}[domain=-4:4]
  \draw[thin,->] (-4.2,0) -- (4.2,0);
  \draw[thin,->] (0,-0.2) -- (0,2.2);
  \draw[dotted] (-4.1,-0.1) grid (4.1,2.1);
  \draw[dashed,thick,color=blue]   plot (\x, {2/(1+exp(-\x))})    node[right] {$f$};
  \draw[dashdotted,thick,color=red]  plot (\x, {2*exp(-\x)/(1+exp(-\x))^2}) node[right] {$\frac{df}{dz}$};
\end{tikzpicture}


*** Softmax layer
- This layer is used for *multi-class classification*
- It is a straightforward generalisation of the sigmoid function.
\[
y_i(\bz) = \frac{\exp(z_i)}{\sum_j \exp(z_j)}
\]
**** Derivative
\[
\frac{\partial}{\partial z_i} y_i (\bz)
=
\frac{e^{z_i} e^{\sum_{j \neq i} z_j}}{\left(\sum_j e^{z_j}\right)^2}
\]

\[
\frac{\partial}{\partial z_i} y_k (\bz)
=
\frac{e^{z_i + z_k}}{\left(\sum_j e^{z_j}\right)^2}
\]
** Cost functions
*** Classification cost functions
**** Col A                                                            :BMCOL:
	 :PROPERTIES:
	 :BEAMER_col: 0.5
	 :END:
***** Classification error
If $z$ is the output for each class then
\[
\cost(z, y) = \ind{y \notin \argmax(z)}
\]
This is not differentiable.
***** Error margin
If $z$ is the positive class output then
\[
\cost(z, y) = - \ind{zy < 0} z y
\]
Used in the perceptron.
***** Negative log likelihood
If $z$ are label probabilities, then 
\[
\cost(z, y) = - \ln z_y.
\]
Used in logistic regression.
**** Col B                                                            :BMCOL:
	 :PROPERTIES:
	 :BEAMER_col: 0.5
	 :END:
[[../fig/class_loss_functions.pdf]]
***** Hinge loss
If $z$ are the output for each class
\[
\cost(z, y) = 1 - z_y
\]
Used in large margin classifiers.
*** Regression cost functions
**** Col A                                                            :BMCOL:
	 :PROPERTIES:
	 :BEAMER_col: 0.5
	 :END:
***** L2 loss (Squared error)
If $z$ is a prediction for $y$ then
\[
\cost(z, y) = (y - z)^2
\]
This is equivalent to negative log likelihood under Gaussianity. Used in linear regression.
***** L1 loss
If $z$ is a prediction for $y$ then
\[
\cost(z, y) = |y - z|
\]
Used in LASSO regression.
**** Col B                                                            :BMCOL:
	 :PROPERTIES:
	 :BEAMER_col: 0.5
	 :END:
[[../fig/reg_loss_functions.pdf]]
***** Huber loss
If $z$ is a prediction, then
\begin{equation}
\cost(z, y) = 
\begin{cases}
\frac{1}{2} (z - y)^2 & |z - y| \leq \delta\\
 \delta(|z - y| - \frac{1}{2} \delta) & \textrm{otherwise.}
\end{cases}
\end{equation}
Mixes L1 and L2 losses.

** Gradient descent
*** Smooth function :B_definition:
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
A function $f : \Reals^d \to \Reals$ is \(\ell\)-smooth if:
\[
\|\nabla_x f(x) - \nabla_y f(y)\|_2 \leq \ell \|x - y\|_2.
\]
*** Contraction mappings :B_definition:
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
A function $f : \Reals^d \to \Reals^d$ is a contraction for some norm $\| \cdot \|$ if
\[
\|f(x) - f(y)\| \leq \|x - y\|.
\]
In other words, it is a contraction if it is 1-Lipschitz. 
In addition, contraction mappings have a fixed point $x^*$ such that
$f(x^*) = x^*$.

*** Gradient descent as a contraction                             :B_theorem:
    :PROPERTIES:
    :BEAMER_env: theorem
    :END:
Suppose $f : \Reals^d \to \Reals$ is convex and \(\ell\)-smooth, 
Then the mapping
\[
\psi(x) \defn x - \eta \nabla_x f(x)
\]
is a contraction as long as $\eta \leq 2/\ell$.

[See Nesterov 04 or Appendix A of Iterative Privacy Amplification for proofs]


** Stochastic gradient descent in practice
*** Gradient descent in practice
**** The ideal gradient descent algorithm:
If we could calculte $\nabla_\vparam \E_\vparam[\loss]$, we could do:
\[
\vparam_{n+1} = \vparam_n - \alpha_n \nabla_\vparam \E_\vparam[\loss]
\]
for a suitable $\alpha_n$ schedule.
**** Gradient descent on the empirical error
Since we only have the data, we can try to minimse the empirical loss
$\frac{1}{T} \sum_{t=1}^T \cost(x_t, y_t, \vparam)$ through gradient descent
\[
\vparam_{n+1} = \vparam_{n} - \alpha_n \frac{1}{T} \sum_{t=1}^T \nabla_\vparam \cost(x_t, y_t, \vparam)
\]
This is also called *batch* gradient descent.
*** Stochastic gradient descent
**** Gradient descent on one example:
We don't have to wait calculate $\nabla_\vparam \cost(x_t, y_t, \vparam)$ for all $t$ before applying the update. We can do it at every example:
\[
\vparam_{n+1}= \vparam_n -  \alpha_n \nabla_\vparam \cost(x_{[n]_T}, y_{[n]_T}, \vparam).
\]
Here $[n]_T$ is 1 + n modulo T to ensure $n \in \{1, \ldots, T\}$.
**** Minibatch gradient descent
However, it is a bit better to look at $K$ examples at a time before we change the parameters. This is called a *minibatch*
\[
\vparam_{n+1}= \vparam_n -  \alpha_n \frac{1}{K} \sum_{k=nK}^{(n+1)K-1} \nabla_\vparam \cost(x_{[k]_T}, y_{[k]_T}, \vparam)
\]
This also helps with parallelisation, since we can compute $\cost, \nabla_\vparam \cost$ in parallel for each example.

* Python libraries
** sklearn
*** sklearn neural networks
**** Classification
Uses the *cross entropy* cost 
#+BEGIN_SRC python
from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(hidden_layer_sizes=(5, 2))
clf.fit(X, y)
clf.predict(X_test)
#+END_SRC
- Main condition is layer sizes.

**** Regression
#+BEGIN_SRC python :exports code
from sklearn.neural_network import MLPRegressor
model = MLPRegressor(hidden_layer_sizes=(5, 2))
#+END_SRC
** PyTorch
*** PyTorch
**** Data set-up
#+BEGIN_SRC python :exports code
X_train = torch.tensor(X_train, dtype=torch.float32)
train_dataset = TensorDataset(X_train, y_train)  
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
#+END_SRC
*** PyTorch: Manual training
**** Network setup
#+BEGIN_SRC python :exports code
fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden layer
fc2 = nn.Linear(hidden_size, output_size)  # Hidden layer to output layer
sigmoid = nn.Sigmoid() # some activation function
criterion = nn.BCELoss() #what loss to minimise
optimizer = optim.SGD(model.parameters(), lr=0.001) # how to minimise it
#+END_SRC
**** Training
#+BEGIN_SRC python :exports code
# Manual forward pass.
z1 = fc1(inputs)  # hidden layer 1
a1 = sigmoid(z1)     # Apply activation for hidden
z2 = fc2(a1)      # Linear combination in output layer
outputs = sigmoid(z2)  # Output layer activation
loss = criterion(outputs, labels) # Specify loss
loss.backward() # Backward pass
optimizer.step() # Update weights
#+END_SRC

** TensorFlow
*** TensorFlow
	This is another library, no need to use this for this course
a
