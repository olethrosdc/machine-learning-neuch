#+TITLE:     Bayesian Inference and Hypothesis Testing
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Params {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \usepackage[bbgreekl]{mathbbol}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:2
#+OPTIONS: toc:nil

* Conditional Probability and the Theorem of Bayes
#+TOC: headlines
** Bayes theorem                                         :theory:probability:
#+ATTR_BEAMER: :overlay <+->
- Recall the definition of Conditional probability:
 \[
 P(A | B) = P(A \cap B) / P(B)
 \]
 i.e. the probability of A happening if B happens.
- It is also true that:
 \[
 P(B | A) = P(A \cap B) / P(A)
 \]
- Combining the two equations, reverse the conditioning:
 \[
 P(A | B) = P(B | A) P (A) / P(B)
 \]

- So we can reverse the order of conditioning, i.e. relate to the probability of A given B to that of B given A.

** The cards problem
 1. Print out a number of cards, with either [A|A], [A|B] or [B|B] on their sides.
 2. If you have an A, what is the probability of an A on the other side?
 3. Have the students perform the experiment with:
    1. Draw a random card.
    2. Count the number of people with A.
    3. What is the probability that somebody with an A on one side will have an A on the other?
    4. Half of the people should have an A?
#+BEAMER: \pause

*** The prior and posterior probabilities
	| A | A | 2/6 | A observed | 2/3
	| A | B | 1/6 | A observed | 1/3
	| B | A | 1/6 |            |
	| B | B | 2/6 |            |

* Simple Bayesian hypothesis testing
#+TOC: headlines
** The murder problem
#+ATTR_BEAMER: :overlay <+->
- Somebody saw somebody matching their description and he was found
       in the neighbourghood. There is no other evidence.

- There are two possibilities:
       - $H_0$: They are innocent.
       - $H_1$: They are guilty.

       What is your belief that they have committed the crime? 
	
*** Prior elicitation
#+ATTR_BEAMER: :overlay <+->
- All those that think the accused is guilty, raise their hand.
- Divide by the number of people in class
- Let us call this $P(H_1)$.
- This is a purely subjective measure!

** DNA test

 - Let us now do a DNA test on the suspect
#+BEAMER: \pause

*** DNA test properties
 #+ATTR_BEAMER: :overlay <+->
 - $D$: Test is positive
 - $P(D | H_0) = 1\%$: False positive rate
 - $P(D | H_1) = 100\%$: True positive rate

#+BEAMER: \pause

*** Run the test
#+ATTR_BEAMER: :overlay <+->
- The result is either positive or negative ($\neg D)$.
- What is your belief *now* that the suspect is guilty?

** Everybody is a suspect
       #+ATTR_BEAMER: :overlay <+->
- Run a DNA test on everybody.
- What is different from before?
- Who has a positive test?
- What is your belief that the people with the positive test are guilty?

** Explanation
       #+ATTR_BEAMER: :overlay <+->
- Prior: $P(H_i)$.
- Likelihood $P(D | H_i)$.
- Posterior: $P(H_i | D) = P(D \cap H_i) / P(D) = P(D | H_i) P(H_i) / P(D)$
- Marginal probability: $P(D) = P(D | H_0) P(H_0) + P(D | H_1) P(H_1)$
- Posterior: $P(H_0 | D) = \frac{P(D | H_0) P(H_0)}{P(D | H_0) P(H_0) + P(D | H_1) P(H_1)}$
- Assuming $P(D | H_1) = 1$, and setting $P(H_0) = q$, this gives
       \[
       P(H_0 | D) = \frac{0.1 q}{0.1 q + 1 - q} =  \frac{q}{10 - 9q}
       \]
- The posterior can always be updated with more data!
** Python example

#+BEGIN_SRC python
# the input to the function is the prior, the likelihood function, and posteriors
# Input:
# - prior for hypothesis 0 (scalar)
# - data (single data point)
# - likelihood[data][hypothesis] array unction
# Returns:
# - posterior for the data point (if multiple points are given, the calculation is repeated)
def get_posterior(prior, data, likelihood):
    marginal = prior * likelihood[data][0] + (1 - prior) * likelihood[data][1]
    posterior = prior * likelihood[data][0] / marginal
    return posterior

import numpy as np
prior = 0.9
likelihood = np.zeros([2, 2])
# pr of negative test if not a match
likelihood[0][0] = 0.9
# pr of positive test if not a match
likelihood[1][0] = 0.1
# pr of negative test if a match
likelihood[0][1] = 0
# pr of positive test if a match
likelihood[1][1] = 1
data = 1
return get_posterior(prior, data, likelihood)
#+END_SRC

#+RESULTS:
: 0.4736842105263158


** Types of hypothesis testing problems
#+ATTR_BEAMER: :overlay <+->
*** Simple Hypothesis Test
#+ATTR_BEAMER: :overlay <+->
Example: DNA evidence, Covid tests
- Two hypothesese $H_0, H_1$
- $P(D | H_i)$ is defined for all $i$

*** Multiple Hypotheses Test
#+ATTR_BEAMER: :overlay <+->
Example: Model selection
- $H_i$: One of many mutually exclusive models
- $P(D | H_i)$ is defined for all $i$

*** Null Hypothesis Test
#+ATTR_BEAMER: :overlay <+->
Example: Are men's and women's heights the same?
- $H_0$: The 'null' hypothesis
- $P(D | H_0)$ is defined
- The alternative is *undefined*

** Pitfalls
#+ATTR_BEAMER: :overlay <+->

*** Problem definition
#+ATTR_BEAMER: :overlay <+->
- Defining the models $P(D | H_i)$ incorrectly.
- Using an "unreasonable" prior $P(H_i)$

*** The garden of many paths
#+ATTR_BEAMER: :overlay <+->
- Having a huge hypothesis space
- Selecting the relevant hypothesis after seeing the data


* Bayesian Inference
** Bayesian Inference

- Model family $\{P_\param |  \param \in \Params\}$
- Each model $\param$ assigns probabilities $P_\param(x)$  to possible $x \in X$.
- We also have a (subjective) prior distribution $\bel$ over the parameters.
- Given $x$, we calculate the posterior distribution
\begin{align}
\bel(\param | x)
& = \frac{P_\param(x) \bel(\param)}{\sum_{\param' \in \Params} P_{\param'}(x) \bel(\param')},
\tag{finite $\Params$}
\\
\bel(\param | x)
& = \frac{P_\param(x) \bel(\param)}{\int_{\Params} P_{\param'}(x) \bel(\param') d\param'},
\tag{continuous $\Params$}
\\
\bel(B | x)
& = \frac{\int_{B} P_{\param'}(x) d\bel(\param)}
{\int_{\Params}P_{\param'}(x) d\bel(\param)},
&& B \subset \Params
\tag{arbitrary $\Params$}
\end{align}
*** Alternative notation for different probability spaces
- The *prior* $\bel(\param) = \Pr(\param)$ and *posterior* $\bel(\param \mid x) = \Pr(\param \mid x)$ belief.
- The *likelihood* $P_\param(x) = \Pr(x \mid \param)$
- The *marginal* $\Pr_\bel(x) = \sum_\param P_\param(x) \bel(\param)$.
** Probabilistic machine learning
*** Setting
- Model family $\{P_\param |  \param \in \Params\}$
- Prior $\bel$ on $\Params$
- Observations $x = x_1, \ldots, x_t$.
*** Maximum likelihood approach
- Model selection: $\param^*_{ML}(x) = \argmax_\param P_\param(x)$.
- Model prediction: $P_{\param^*_{ML}(x)}(x_{t+1})$ 
*** Maximum a posteriori approach
- Model selection: $\param^*_{MAP}(x) = \argmax_\param P_\param(x) \bel(\param)$.
- Model prediction: $P_{\param^*_{MAP}(x)}(x_{t+1})$ 
*** Bayesian approach
- Posterior calculation: $\bel(\param | x) = P_\param(x) \bel(\param) / \Pr_\bel(x)$
- Model prediction: $\Pr_\bel(x_{t+1} | x) = \sum_\param P_\param(x_{t+1}) \bel(\param | x)$ 
** Differences between approaches
*** Maximum likelihood approach
- Ignores model complexity
- Is an optimisation problem
*** Maximum a posteriori approach
- Regularises model selection using the prior
- Can be seen as solving the optimisation problem
  \[
  \max_\param \ln P_\param(x) + \ln \bel(\param),
  \]
  where the prior term $\ln \bel(\param)$ acts as a regulariser.
*** Bayesian approach
- Does not select a single model
- Averages over all models according to their fit *and* the prior
- Does *not* result in an optimisation problem.


** The $n$-meteorologists problem  
- Consider $n$ meteorological stations predicting rainfall
- $x_t \in \{0,1\}$ with $x_t = 1$ if it rains on day $t$.
- We have a prior distribution $\bel(\mu)$ for each station.
- At time $t$, station $\mu$ makes as a prediction $P_\mu(x_{t+1} | x_1, \ldots, x_t)$
- We observe $x_{t+1}$ and calculate the posterior  $\bel(\mu | x_1, \ldots, x_t, x_{t+1})$.
*** The marginal distribution 
To take into account all stations, we can marginalise:
\[
\Pr_\bel(x_{t+1} \mid x_1, \ldots x_t) = 
\sum_\mu P_\mu(x_{t+1} | x_t) \bel(\mu)
\]
*** The posterior :exercise:
- Show that
\[
\bel(\mu \mid x_1, \ldots, x_{t+1}) = 
\frac{P_\mu(x_t \mid x_1, \ldots, x_t) \bel(\mu|x_1, \ldots, x_t)}
{\sum_{\mu'} P_{\mu'}(x_t \mid x_1, \ldots, x_t) \bel(\mu'|x_1, \ldots, x_t)}
\]
- How would you implement an ML or a MAP solution to this problem?

** Sufficient statistics
*** A statistic $f$
This is any function $f : X \to S$ where
- $X$ is the data space
- $S$ is an arbitrary space
*** Example statistics for $X = \Reals^*$ (the set of all real-valued sequences)
- The sample mean of a sequence $1/T \sum_{t=1}^T x_t$
- The total number of samples $T$
*** Sufficient statistic
$f$ is sufficient for a family $\{P_\param : \param \in \Params\}$ when
\[
f(x) = f(x') \Rightarrow P_\param(x) = P_\param(x') \forall \param \in \Params.
\]
If there exists a finite-dimensional sufficient statistic, Bayesian and ML learning can be done in closed form within the family.
** Conjugate priors
Consider a parametrised family of priors $\Bels$ on $\Params$ and a distribution family $\{P_\param\}$
The pair is conjugate if, for any prior $\bel \in \Bels$, and any observation $x$, there exists $\bel' \in \Bels$ such that $\bel'(\param) = \bel(\param | x)$
*** Standard Parametric conjugate families
|---------------+------------+---------------------------------+-----------------------|
| Prior         | Likelihood | Parameters $\param$             | Observations $x$      |
|---------------+------------+---------------------------------+-----------------------|
| Beta         | Bernoulli  | $[0,1]$                         | $\{0,1\}^T$           |
| Multinomial  | Dirichlet  | $\Simplex^n$                    | $\{1, \ldots, n\}^T$  |
| Gamma        | Normal     | $\Reals, \Reals$                | $\Reals^T$            |
| Wishart      | Normal     | $\Reals^n, \Reals^{n \times n}$ | $\Reals^{n \times T}$ |
|---------------+------------+---------------------------------+-----------------------|

The Simplex $\Simplex^n = \{\vparam \in [0,1]^n : \|\vparam\|_1\}$ is the set of all \(n\)-dimensional probability vectors.

*** Extensions
- Discrete Bayesian Networks.
- Linear-Gaussian Models (i.e. Bayesian linear regression)
- Gaussian Processes.

** Beta-Bernoulli
\begin{tikzpicture}
\node[RV] at (1,0) (x) {$x_t$};
\node[RV,hidden] at (0,0) (mean) {$\theta$};
\node[RV] at (-1,0) (prior) {$\vectorsym{\alpha}$};
\draw[->] (prior) to (mean);
\draw[->] (mean) to (x);
\end{tikzpicture}

*** Definition of the Bernoulli distribution
If $x_t \mid \param \sim \Ber(\param)$. $\param \in [0,1]$, $x_t \in \{0, 1\}$ and:
\[
P_\param(x_t = 1) = \param
\]
*** Definition of the Beta density 
If $\param \sim \Beta(\alpha_1, \alpha_0)$, $\alpha_0, \alpha_1 > 0$ and
\[
p(\param | \alpha_1, \alpha_0) \propto \param^{\alpha_1 - 1} (1 - \param)^{\alpha_0 - 1}
\]
*** Beta-Bernoulli conjugate pair
- $\param \sim \Beta(\alpha_1, \alpha_0)$.
- $x_t \mid \param \sim \Ber(\param)$.
Then, for any $x = x_1, \ldots, x_T$, the posterior distribution is
- $\param \mid x \sim \Beta(\alpha_1 + \sum_t x_t , \alpha_0 + T - \sum_t x_t)$.
** Dirichlet-Multinomial
\begin{tikzpicture}
\node[RV] at (1,0) (x) {$x_t$};
\node[RV,hidden] at (0,0) (mean) {$\vparam$};
\draw[->] (mean) to (x);
\node[RV] at (-1,0) (prior) {$\vectorsym{\alpha}$};
\draw[->] (prior) to (mean);
\end{tikzpicture}
*** Definition of the Multinomial distribution
If $x_t \mid \vparam \sim \Mult(\vparam)$,
with $\param \in \Simplex^n$ and $x_t \in \{1, \ldots, n\}$ and:
\[
P_\vparam(x_t = i) = \param_i
\]
*** Definition of the Dirichlet density 
If $\vparam \sim \Dir(\vectorsym{\alpha})$, with $\vectorsym{\alpha} \in \Reals^n_+$ then
\[
p(\param | \vectorsym{\alpha}) \propto \prod_i \param_i^{\alpha_i - 1}
\]
*** Dirichlet-Multinomial conjugate pair
- $\param \sim \Dir(\vectorsym{\alpha})$.
- $x_t \mid \param \sim \Ber(\vparam)$.
Then, for any $x = x_1, \ldots, x_T$, the posterior distribution is
- $\param \mid x \sim \Dir(\vectorsym{\alpha + \vectorsym{s}_T})$, where $s_{T,i} = \sum_{t=1}^T \ind{x_t = i}$,

** Discrete Bayesian Networks
\begin{tikzpicture}
\node[RV] at (0,0) (x1) {$x_1$};
\node[RV] at (0,1) (x2) {$x_2$};
\node[RV] at (1,0) (x3) {$x_3$};
\node[RV] at (1,1) (x4) {$x_4$};
\node[RV,hidden] at (-1,0) (m1) {$\vparam_1$};
\node[RV,hidden] at (-1,1) (m2) {$\vparam_2$};
\node[RV,hidden] at (2,0) (m3) {$\vparam_3$};
\node[RV,hidden] at (2,1) (m4) {$\vparam_4$};
\draw[->] (x1) to (x2);
\draw[->] (x2) to (x3);
\draw[->] (x4) to (x3);
\draw[->] (x2) to (x4);
\draw[->] (m1) to (x1);
\draw[->] (m2) to (x2);
\draw[->] (m3) to (x3);
\draw[->] (m4) to (x4);
\end{tikzpicture}

- A directed acyclic graph (DAG) defined on variables $x_1, \ldots, x_n$ with each $x_n$ taking a finite number of values,
- Let $S_i$ be the indices corresponding to parent variables of $x_i$.
- $x_i \mid \vparam_i, x_{S_i} = k \sim \Mult(\vparam_{i,k})$.

*** Example: Lung cancer, smoking and asbestos
**** LSA DAG
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:

\begin{tikzpicture}
\node[RV] at (0,0) (x1) {$x_S$};
\node[RV] at (0,1) (x2) {$x_C$};
\node[RV] at (1,0) (x3) {$x_A$};
\node[RV,hidden] at (-1,0) (m1) {$\param_A$};
\node[RV,hidden] at (-1,1) (m2) {$\vparam_C$};
\node[RV,hidden] at (2,0) (m3) {$\param_S$};
\draw[->] (x1) to (x2);
\draw[->] (x3) to (x2);
\draw[->] (m1) to (x1);
\draw[->] (m2) to (x2);
\draw[->] (m3) to (x3);
\end{tikzpicture}
**** LSA Equations
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
\begin{align}
P_{\param_A}(x_A = 1) &= \param_A\\
P_{\param_S}(x_S = 1) &= \param_S\\
P_{\vparam_C}(x_C = 1 \mid X_A= j, X_S = k) &= \param_{C,j,k}
\end{align}

** Markov model
\begin{tikzpicture}
\node[RV] at (-1,0) (x0) {$x_{t-1}$};
\node[RV] at (0,0) (x1) {$x_t$};
\node[RV] at (1,0) (x2) {$x_{t+1}$};
\node[RV,hidden] at (1,1) (m1) {$\vparam$};
\node[RV] at (0,1) (prior) {$\vectorsym{\alpha}$};
\draw[->] (prior) to (m1);
\draw[->] (m1) to (x0);
\draw[->] (m1) to (x1);
\draw[->] (m1) to (x2);
\draw[->] (x0) to (x1);
\draw[->] (x1) to (x2);
\end{tikzpicture}

A *Markov model* obeys
\[
\Pr_\vparam(x_{k+1} | x_k, \ldots, x_1) = \Pr_\vparam(x_{k+1} | x_k)
\]
i.e. the graphical model is a chain. We are usually interested in *homogeneous* models, where
\[
\Pr_\vparam(x_{k+1} = i \mid x_k = j) = \param_{i,j} \qquad \forall k
\]
*** Inference for finite Markov models
- If $x_t \in [n]$ then $x_{t+1} \mid \vparam, x_t = i \sim \Mult(\vparam_i)$, $\vparam_i \in \Simplex^n$
- Prior $\vparam_i \mid \vectorsym{\alpha} \sim \Dir(\vectorsym{\alpha})$ for all $i \in [n]$.
- Posterior $\vparam_i \mid x_1, \ldots, x_t, \vectorsym{\alpha} \sim \Dir(\vectorsym{\alpha}^{(t)})$ with
  \[
  \alpha^{t}_{i,j} = \alpha_{i,j} + \sum_{k=1}^t \ind{x_k = i \wedge x_{k+1} = j},
  \qquad
  \vectorsym{\alpha}^0 =   \vectorsym{\alpha}.
  \]

